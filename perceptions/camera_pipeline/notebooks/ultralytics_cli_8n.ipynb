{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO Training on FSOCO with Ultralytics CLI\n",
    "\n",
    "**Essential steps only:**\n",
    "1. Setup Ultralytics (YOLOv8)\n",
    "2. Convert FSOCO labels (Supervisely YOLO)\n",
    "3. Train\n",
    "4. Inference\n",
    "\n",
    "**Run from:** `driverless-ml-dev/perceptions/camera-pipeline/notebooks/`\n",
    "\n",
    "**Activate venv in root/driverless-ml-dev directory, and connect kernel for this notebook to it**\n",
    "open vsc terminal via ctrl+shift+`\n",
    "\n",
    "first time only:\n",
    "```\n",
    "cd ~/driverless-ml-dev\n",
    "python -m venv venv\n",
    "source venv/bin/activate\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --name driverless-ml --display-name \"Python (driverless-ml)\" --user\n",
    "```\n",
    "then type `>developer reload` in the search bar at the top\n",
    "\n",
    "to activate venv\n",
    "```\n",
    "cd /root/driverless-ml-dev\n",
    "source venv/bin/activate\n",
    "```\n",
    "then type `>developer reload` in the search bar at the top\n",
    "\n",
    "install libraries with: `pip install ultralytics pillow tqdm pyyaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Ultralytics Installation\n",
    "\n",
    "Verify that Ultralytics is installed and CUDA is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
      "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "Ultralytics 8.3.205 üöÄ Python-3.12.3 torch-2.8.0a0+34c6371d24.nv25.08 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "Setup complete ‚úÖ (16 CPUs, 7.6 GB RAM, 111.6/1006.9 GB disk)\n",
      "\n",
      "OS                     Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.39\n",
      "Environment            Docker\n",
      "Python                 3.12.3\n",
      "Install                pip\n",
      "Path                   /usr/local/lib/python3.12/dist-packages/ultralytics\n",
      "RAM                    7.61 GB\n",
      "Disk                   111.6/1006.9 GB\n",
      "CPU                    13th Gen Intel Core i7-13620H\n",
      "CPU count              16\n",
      "GPU                    NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB\n",
      "GPU count              1\n",
      "CUDA                   13.0\n",
      "\n",
      "numpy                  ‚úÖ 1.26.4>=1.23.0\n",
      "matplotlib             ‚úÖ 3.10.5>=3.3.0\n",
      "opencv-python          ‚úÖ 4.11.0.86>=4.6.0\n",
      "pillow                 ‚úÖ 11.3.0>=7.1.2\n",
      "pyyaml                 ‚úÖ 6.0.2>=5.3.1\n",
      "requests               ‚úÖ 2.32.4>=2.23.0\n",
      "scipy                  ‚úÖ 1.15.3>=1.4.1\n",
      "torch                  ‚úÖ 2.8.0a0+34c6371d24.nv25.8>=1.8.0\n",
      "torch                  ‚úÖ 2.8.0a0+34c6371d24.nv25.8!=2.4.0,>=1.8.0; sys_platform == \"win32\"\n",
      "torchvision            ‚úÖ 0.23.0a0+428a54c9>=0.9.0\n",
      "psutil                 ‚úÖ 7.0.0\n",
      "polars                 ‚úÖ 1.28.1\n",
      "ultralytics-thop       ‚úÖ 2.0.17>=2.0.0\n"
     ]
    }
   ],
   "source": [
    "!yolo checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to create and install a virtual environment\n",
    "# %cd ~/driverless-ml-dev\n",
    "# !python -m venv venv\n",
    "# !source venv/bin/activate\n",
    "# !venv/bin/python -m pip install --upgrade pip ipykernel\n",
    "# !venv/bin/python -m ipykernel install --name driverless-ml --display-name \"Python (driverless-ml)\" --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activate venv in root/driverless-ml-dev directory, and connect kernel for this notebook to it**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.3.205)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.2)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.5)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.11.0.86)\n",
      "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.15.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0a0+34c6371d24.nv25.8)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0a0+428a54c9)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (7.0.0)\n",
      "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.28.1)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.17)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (23.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.7.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (79.0.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install ultralytics pillow tqdm pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/driverless-ml-dev\n",
      "Root: /root/driverless-ml-dev\n",
      "Data: /root/driverless-ml-dev/ml_data\n",
      "Fsoco_raw: /root/driverless-ml-dev/ml_data/fsoco_raw\n",
      "Fsoco_mod: /root/driverless-ml-dev/ml_data/fsoco_mod\n",
      "Fsoco_yolo: /root/driverless-ml-dev/ml_data/fsoco_yolo\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "\n",
    "# Paths\n",
    "ROOT = Path.cwd().parent.parent.parent  # driverless-ml-dev/\n",
    "print(ROOT)\n",
    "DATA_DIR = ROOT / 'ml_data'\n",
    "FSOCO_RAW = ROOT / 'ml_data/fsoco_raw'  # download fsoco dataset\n",
    "FSOCO_MOD = ROOT / 'ml_data/fsoco_mod'  # copy of raw for preprocessing\n",
    "FSOCO_YOLO = ROOT / 'ml_data/fsoco_yolo'  # final dataset\n",
    "\n",
    "print(f\"Root: {ROOT}\")\n",
    "print(f\"Data: {DATA_DIR}\")\n",
    "print(f\"Fsoco_raw: {FSOCO_RAW}\")\n",
    "print(f\"Fsoco_mod: {FSOCO_MOD}\")\n",
    "print(f\"Fsoco_yolo: {FSOCO_YOLO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download FSOCO Dataset\n",
    "\n",
    "**Manual step required:**\n",
    "1. Visit: https://fsoco.github.io/fsoco-dataset/download\n",
    "2. Download Bounding Boxes dataset (24GB)\n",
    "3. Extract to: `root/ml-data/perceptions/`\n",
    "\n",
    "Expected structure:\n",
    "```\n",
    "root/ml-data/perceptions/\n",
    "‚îî‚îÄ‚îÄ fsoco_raw/\n",
    "    ‚îú‚îÄ‚îÄ team1/ann/       # JSON annotations\n",
    "    ‚îú‚îÄ‚îÄ team1/img/       # Images\n",
    "    ‚îî‚îÄ‚îÄ meta.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FSOCO dataset found\n",
      "Skipping flatten: working copy already populated /root/driverless-ml-dev/ml_data/fsoco_mod\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def copy_and_flatten_dataset(source_path: Path, dest_path: Path):\n",
    "    \"\"\"\n",
    "    flattening raw dataset:\n",
    "    source/\n",
    "      - meta.json\n",
    "      - teamA/{ann,img}\n",
    "      - teamB/{ann,img}\n",
    "      ...\n",
    "    ->\n",
    "    dest/\n",
    "      - ann/\n",
    "      - img/\n",
    "      - meta.json\n",
    "    \"\"\"\n",
    "    if not source_path.exists() or not any(source_path.iterdir()):\n",
    "        raise FileNotFoundError(f\"Source dataset not found or empty: {source_path}\")\n",
    "\n",
    "    if dest_path.exists() and any(dest_path.iterdir()):\n",
    "        print(f\"Skipping copy: destination already populated {dest_path}\")\n",
    "        return\n",
    "\n",
    "    dest_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for item in dest_path.iterdir():\n",
    "        if item.is_dir():\n",
    "            shutil.rmtree(item)\n",
    "        else:\n",
    "            item.unlink()\n",
    "\n",
    "    ann_out = dest_path / \"ann\"\n",
    "    img_out = dest_path / \"img\"\n",
    "    ann_out.mkdir(exist_ok=True)\n",
    "    img_out.mkdir(exist_ok=True)\n",
    "\n",
    "    meta_src = source_path / \"meta.json\"\n",
    "    if meta_src.exists():\n",
    "        shutil.copy2(meta_src, dest_path / \"meta.json\")\n",
    "\n",
    "    ann_count = 0\n",
    "    img_count = 0\n",
    "\n",
    "    for team_dir in source_path.iterdir():\n",
    "        print(f\"Processing team directory: {team_dir.name}\")\n",
    "        if not team_dir.is_dir():\n",
    "            print(f\"Skipping non-directory item: {team_dir.name}\")\n",
    "            continue\n",
    "\n",
    "        if team_dir.name in {\"ann\", \"img\"}:\n",
    "            continue\n",
    "\n",
    "        ann_dir = team_dir / \"ann\"\n",
    "        img_dir = team_dir / \"img\"\n",
    "\n",
    "        if ann_dir.exists():\n",
    "            for f in ann_dir.iterdir():\n",
    "                if f.is_file():\n",
    "                    dest = ann_out / f\"{team_dir.name}_{f.name}\"\n",
    "                    shutil.copy2(f, dest)\n",
    "                    ann_count += 1\n",
    "\n",
    "        if img_dir.exists():\n",
    "            for f in img_dir.iterdir():\n",
    "                if f.is_file():\n",
    "                    dest = img_out / f\"{team_dir.name}_{f.name}\"\n",
    "                    shutil.copy2(f, dest)\n",
    "                    img_count += 1\n",
    "\n",
    "    print(f\"Copy+flatten complete {ann_out} ({ann_count} files), {img_out} ({img_count} files)\")\n",
    "\n",
    "\n",
    "raw_exists = FSOCO_RAW.exists()\n",
    "raw_has_content = os.path.isdir(FSOCO_RAW) if raw_exists else False\n",
    "\n",
    "# used later on to skip ratio filtering since it assumes you already ran it\n",
    "mod_exists = FSOCO_MOD.exists()\n",
    "if raw_exists and raw_has_content:\n",
    "    print(\"FSOCO dataset found\")\n",
    "    if FSOCO_MOD.exists() and any(FSOCO_MOD.iterdir()):\n",
    "        print(f\"Skipping flatten: working copy already populated {FSOCO_MOD}\")\n",
    "    else:\n",
    "        for item in FSOCO_RAW.iterdir():\n",
    "            print(f\"  - {item.name}\")\n",
    "        FSOCO_MOD.mkdir(parents=True, exist_ok=True)\n",
    "        copy_and_flatten_dataset(FSOCO_RAW, FSOCO_MOD)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Download FSOCO dataset to: {FSOCO_RAW}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting dataset at: /root/driverless-ml-dev/ml_data/fsoco_mod\n",
      "/root/driverless-ml-dev/ml_data/fsoco_mod/ann\n",
      "ann contents:\n",
      "  amz_amz_00000.jpg.json\n",
      "  amz_amz_00001.jpg.json\n",
      "  amz_amz_00002.jpg.json\n",
      "  amz_amz_00003.jpg.json\n",
      "  amz_amz_00004.jpg.json\n",
      "\n",
      "img contents:\n",
      "  amz_amz_00000.jpg\n",
      "  amz_amz_00001.jpg\n",
      "  amz_amz_00002.jpg\n",
      "  amz_amz_00003.jpg\n",
      "  amz_amz_00004.jpg\n",
      "\n",
      "meta.json:\n",
      "{\n",
      "  \"classes\": [\n",
      "    {\n",
      "      \"title\": \"seg_orange_cone\",\n",
      "      \"shape\": \"bitmap\",\n",
      "      \"color\": \"#FF8000\",\n",
      "      \"geometry_config\": {},\n",
      "      \"id\": 9993505,\n",
      "      \"hotkey\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"unknown_cone\",\n",
      "      \"shape\": \"rectangle\",\n",
      "      \"color\": \"#3BDB0F\",\n",
      "      \"geometry_config\": {},\n",
      "      \"id\": 9993514,\n",
      "      \"hotkey\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"yellow_cone\",\n",
      "      \"shape\": \"rectangle\",\n",
      "      \"color\": \"#FFFF00\",\n",
      "      \"geometry_config\": {},\n",
      "      \"id\": 9993506,\n",
      "      \"ho\n"
     ]
    }
   ],
   "source": [
    "if not FSOCO_MOD.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Working dataset directory not found: {FSOCO_MOD}. Run the preparation step and ensure the directory exists.\"\n",
    "    )\n",
    "\n",
    "if not any(FSOCO_MOD.iterdir()):\n",
    "    raise RuntimeError(\n",
    "        f\"Working dataset directory is empty: {FSOCO_MOD}. Run the preparation step to copy data before proceeding.\"\n",
    "    )\n",
    "\n",
    "working_dataset = FSOCO_MOD\n",
    "print(f\"Inspecting dataset at: {working_dataset}\")\n",
    "\n",
    "bb_path = working_dataset / 'ann'\n",
    "print(bb_path)\n",
    "print(\"ann contents:\")\n",
    "if bb_path.exists():\n",
    "    items = list(os.listdir(bb_path))[:5]\n",
    "    for item in items:\n",
    "        print(f\"  {item}\")\n",
    "else:\n",
    "    print(\"no ann directory found\")\n",
    "\n",
    "img_path = working_dataset / 'img'\n",
    "\n",
    "print(\"\\nimg contents:\")\n",
    "if img_path.exists():\n",
    "    items = list(os.listdir(img_path))[:5]\n",
    "    for item in items:\n",
    "        print(f\"  {item}\")\n",
    "else:\n",
    "    print(\"no image directory found\")\n",
    "\n",
    "print(\"\\nmeta.json:\")\n",
    "meta_path = working_dataset / 'meta.json'\n",
    "\n",
    "if meta_path.exists():\n",
    "    import json\n",
    "    meta = json.load(open(meta_path))\n",
    "    print(json.dumps(meta, indent=2)[:500])\n",
    "else:\n",
    "    print(\"meta.json not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filter Images by Aspect Ratio\n",
    "\n",
    "Remove images that don't meet minimum aspect ratio requirements (width/height)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping aspect ratio filtering: already applied previously.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "def filter_images_by_aspect_ratio(fsoco_mod, min_ratio=1.0, max_ratio=3.0):\n",
    "    img_dir = fsoco_mod / 'images'\n",
    "    if not img_dir.exists():\n",
    "        img_dir = fsoco_mod / 'img'\n",
    "    ann_dir = fsoco_mod / 'bounding_boxes'\n",
    "    if not ann_dir.exists():\n",
    "        ann_dir = fsoco_mod / 'ann'\n",
    "    \n",
    "    if not img_dir.exists():\n",
    "        print(f\"Image directory not found: {img_dir}\")\n",
    "        return [], 0\n",
    "    \n",
    "    images = list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png'))\n",
    "    print(f\"Total images found: {len(images)}\")\n",
    "    \n",
    "    filtered_images = []\n",
    "    removed_images = []\n",
    "    kept_dimensions = []\n",
    "    removed_dimensions = []\n",
    "    kept_aspect_ratios = []\n",
    "    removed_aspect_ratios = []\n",
    "    \n",
    "    for img_path in tqdm(images, desc=\"Filtering images by aspect ratio\"):\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            w, h = img.size\n",
    "            aspect_ratio = w / h\n",
    "            \n",
    "            if min_ratio <= aspect_ratio <= max_ratio:\n",
    "                filtered_images.append(img_path)\n",
    "                kept_dimensions.append((w, h))\n",
    "                kept_aspect_ratios.append(aspect_ratio)\n",
    "            else:\n",
    "                removed_images.append((img_path, aspect_ratio, w, h))\n",
    "                removed_dimensions.append((w, h))\n",
    "                removed_aspect_ratios.append(aspect_ratio)\n",
    "                \n",
    "                img_path.unlink()\n",
    "                \n",
    "                ann_path = ann_dir / f\"{img_path.name}.json\"\n",
    "                if not ann_path.exists():\n",
    "                    ann_path = ann_dir / f\"{img_path.stem}.json\"\n",
    "                if ann_path.exists():\n",
    "                    ann_path.unlink()\n",
    "                \n",
    "                print(f\"Removed: {img_path.name} (aspect ratio: {aspect_ratio:.2f}, dims: {w}x{h})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path.name}: {e}\")\n",
    "            continue\n",
    "    #- Print detailed statistics -\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FILTERING STATISTICS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"\\nOVERALL SUMMARY:\")\n",
    "    print(f\"  Total images processed:  {len(images)}\")\n",
    "    if images:\n",
    "        print(f\"  Images kept:           {len(filtered_images)} ({100*len(filtered_images)/len(images):.1f}%)\")\n",
    "        print(f\"  Images filtered out:   {len(removed_images)} ({100*len(removed_images)/len(images):.1f}%)\")\n",
    "    else:\n",
    "        print(\"  Images kept:           0\")\n",
    "        print(\"  Images filtered out:   0\")\n",
    "    print(f\"  Aspect ratio range:      {min_ratio} - {max_ratio}\")\n",
    "    \n",
    "    if kept_aspect_ratios:\n",
    "        print(f\"\\nKEPT IMAGES - ASPECT RATIO STATS:\")\n",
    "        print(f\"  Min aspect ratio:  {min(kept_aspect_ratios):.2f}\")\n",
    "        print(f\"  Max aspect ratio:  {max(kept_aspect_ratios):.2f}\")\n",
    "        print(f\"  Avg aspect ratio:  {sum(kept_aspect_ratios)/len(kept_aspect_ratios):.2f}\")\n",
    "    \n",
    "    if kept_dimensions:\n",
    "        print(f\"\\nKEPT IMAGES - DIMENSION STATS:\")\n",
    "        dim_counter = Counter(kept_dimensions)\n",
    "        top_dims = dim_counter.most_common(10)\n",
    "        print(f\"  Unique dimensions: {len(dim_counter)}\")\n",
    "        print(f\"  Top 10 dimensions:\")\n",
    "        for (w, h), count in top_dims:\n",
    "            ratio = w/h\n",
    "            print(f\"    {w}x{h} (ratio {ratio:.2f}): {count} images\")\n",
    "    \n",
    "    if removed_aspect_ratios:\n",
    "        print(f\"\\nREMOVED IMAGES - ASPECT RATIO STATS:\")\n",
    "        print(f\"  Min aspect ratio:  {min(removed_aspect_ratios):.2f}\")\n",
    "        print(f\"  Max aspect ratio:  {max(removed_aspect_ratios):.2f}\")\n",
    "        print(f\"  Avg aspect ratio:  {sum(removed_aspect_ratios)/len(removed_aspect_ratios):.2f}\")\n",
    "    \n",
    "    if removed_dimensions:\n",
    "        print(f\"\\nREMOVED IMAGES - DIMENSION STATS:\")\n",
    "        dim_counter = Counter(removed_dimensions)\n",
    "        print(f\"  Unique dimensions: {len(dim_counter)}\")\n",
    "        print(f\"  All removed dimensions:\")\n",
    "        for (w, h), count in sorted(dim_counter.items(), key=lambda x: x[1], reverse=True):\n",
    "            ratio = w/h\n",
    "            print(f\"    {w}x{h} (ratio {ratio:.2f}): {count} images\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "    \n",
    "    return filtered_images, len(removed_images)\n",
    "\n",
    "MIN_ASPECT_RATIO = 1.25  # minimum width/height ratio\n",
    "MAX_ASPECT_RATIO = 1.80  # maximum width/height ratio\n",
    "\n",
    "if not FSOCO_MOD.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Working dataset directory not found: {FSOCO_MOD}. Create it and rerun the preparation step before filtering.\"\n",
    "    )\n",
    "\n",
    "if not any(FSOCO_MOD.iterdir()):\n",
    "    raise RuntimeError(\n",
    "        f\"Working dataset directory is empty: {FSOCO_MOD}. Run the preparation step to copy data before filtering.\"\n",
    "    )\n",
    "\n",
    "if not mod_exists:\n",
    "    print(f\"Using working dataset: {FSOCO_MOD}\")\n",
    "    filtered_imgs, removed = filter_images_by_aspect_ratio(\n",
    "        FSOCO_MOD,\n",
    "        min_ratio=MIN_ASPECT_RATIO,\n",
    "        max_ratio=MAX_ASPECT_RATIO\n",
    "    )\n",
    "    mod_exists = True\n",
    "    print(\"Aspect ratio filtering complete. Flag updated to skip on next run.\")\n",
    "else:\n",
    "    print(\"Skipping aspect ratio filtering: already applied previously.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convert Labels: Supervisely YOLO\n",
    "\n",
    "YOLO format: `<class_id> <x_center> <y_center> <width> <height>` (all normalized 0-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 classes from meta.json: ['unknown_cone', 'yellow_cone', 'blue_cone', 'orange_cone', 'large_orange_cone']\n",
      "Skipping conversion: YOLO dataset already populated /root/driverless-ml-dev/ml_data/fsoco_yolo\n",
      "\n",
      "Final classes list (5 classes): ['unknown_cone', 'yellow_cone', 'blue_cone', 'orange_cone', 'large_orange_cone']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "def convert_supervisely_to_yolo(fsoco_raw, fsoco_yolo, split=(0.8, 0.1, 0.1)):\n",
    "    for s in ['train', 'val', 'test']:\n",
    "        (fsoco_yolo / 'images' / s).mkdir(parents=True, exist_ok=True)\n",
    "        (fsoco_yolo / 'labels' / s).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # hard code class names\n",
    "    classes = ['blue_cone', 'unknown_cone', 'orange_cone', 'large_orange_cone', 'yellow_cone']\n",
    "    class_map = {name: i for i, name in enumerate(classes)}\n",
    "    print(f\"Classes (bounding boxes only): {classes}\\n\")\n",
    "    print(f\"Class mapping: {class_map}\\n\")\n",
    "\n",
    "    ann_dir = fsoco_raw / 'ann'\n",
    "    img_dir = fsoco_raw / 'img'\n",
    "    \n",
    "    ann_files = list(ann_dir.glob('*.json'))\n",
    "    print(f\"Total annotations: {len(ann_files)}\")\n",
    "    \n",
    "    random.shuffle(ann_files)\n",
    "    n1 = int(len(ann_files) * split[0])\n",
    "    n2 = int(len(ann_files) * (split[0] + split[1]))\n",
    "    splits = {\n",
    "        'train': ann_files[:n1],\n",
    "        'val': ann_files[n1:n2],\n",
    "        'test': ann_files[n2:]\n",
    "    }\n",
    "    \n",
    "    for split_name, anns in splits.items():\n",
    "        print(f\"\\nConverting {split_name}: {len(anns)} images\")\n",
    "        \n",
    "        for ann_path in tqdm(anns):\n",
    "            img_name = ann_path.stem\n",
    "            img_path = img_dir / img_name\n",
    "            \n",
    "            if not img_path.exists():\n",
    "                alt_name = img_name.replace('.jpg', '.png') if '.jpg' in img_name else img_name.replace('.png', '.jpg')\n",
    "                img_path = img_dir / alt_name\n",
    "                if not img_path.exists():\n",
    "                    continue\n",
    "\n",
    "            img = Image.open(img_path)\n",
    "            w, h = img.size\n",
    "\n",
    "            ann = json.load(open(ann_path))\n",
    "\n",
    "            yolo_labels = []\n",
    "            for obj in ann.get('objects', []):\n",
    "                cls = obj['classTitle']\n",
    "                if cls not in class_map or obj['geometryType'] != 'rectangle':\n",
    "                    continue\n",
    "                \n",
    "                points = obj['points']['exterior']\n",
    "                x1, y1 = points[0]\n",
    "                x2, y2 = points[1]\n",
    "\n",
    "                x_center = ((x1 + x2) / 2) / w\n",
    "                y_center = ((y1 + y2) / 2) / h\n",
    "                width = abs(x2 - x1) / w\n",
    "                height = abs(y2 - y1) / h\n",
    "\n",
    "                x_center = max(0, min(1, x_center))\n",
    "                y_center = max(0, min(1, y_center))\n",
    "                width = max(0, min(1, width))\n",
    "                height = max(0, min(1, height))\n",
    "                \n",
    "                yolo_labels.append(f\"{class_map[cls]} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\")\n",
    "            \n",
    "            if yolo_labels:\n",
    "                shutil.copy(img_path, fsoco_yolo / 'images' / split_name / img_path.name)\n",
    "                with open(fsoco_yolo / 'labels' / split_name / f\"{img_path.stem}.txt\", 'w') as f:\n",
    "                    f.write('\\n'.join(yolo_labels))\n",
    "    \n",
    "    print(\"\\nConversion complete\")\n",
    "    print(f\"\\nDataset splits:\")\n",
    "    for split_name in ['train', 'val', 'test']:\n",
    "        n_imgs = len(list((fsoco_yolo / 'images' / split_name).glob('*.[jp][pn]g')))\n",
    "        print(f\"  {split_name}: {n_imgs} images\")\n",
    "    \n",
    "    return classes\n",
    "\n",
    "classes = None\n",
    "if (FSOCO_MOD / 'meta.json').exists():\n",
    "    try:\n",
    "        meta = json.load(open(FSOCO_MOD / 'meta.json'))\n",
    "        classes = [c['title'] for c in meta['classes'] if c['shape'] == 'rectangle']\n",
    "        print(f\"Loaded {len(classes)} classes from meta.json: {classes}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to load classes from meta.json: {e}\")\n",
    "\n",
    "yolo_exists = FSOCO_YOLO.exists() and any(FSOCO_YOLO.iterdir())\n",
    "\n",
    "if not (FSOCO_MOD.exists() and any(FSOCO_MOD.iterdir())):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Working dataset not found or empty: {FSOCO_MOD}\\n\"\n",
    "        f\"Please run the dataset preparation step first (cell 10).\"\n",
    "    )\n",
    "\n",
    "if yolo_exists:\n",
    "    print(f\"Skipping conversion: YOLO dataset already populated {FSOCO_YOLO}\")\n",
    "else:\n",
    "    print(f\"Using working dataset: {FSOCO_MOD}\")\n",
    "    classes = convert_supervisely_to_yolo(FSOCO_MOD, FSOCO_YOLO)\n",
    "\n",
    "if classes is None:\n",
    "    raise RuntimeError(\n",
    "        \"Failed to load class definitions.\\n\"\n",
    "        f\"Ensure meta.json exists in {FSOCO_MOD} or run the conversion.\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nFinal classes list ({len(classes)} classes): {classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Dataset Config (YAML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config already exists: /root/driverless-ml-dev/ml_data/fsoco_yolo/fsoco.yaml\n",
      "names:\n",
      "- unknown_cone\n",
      "- yellow_cone\n",
      "- blue_cone\n",
      "- orange_cone\n",
      "- large_orange_cone\n",
      "nc: 5\n",
      "path: /root/driverless-ml-dev/ml_data/fsoco_yolo\n",
      "test: images/test\n",
      "train: images/train\n",
      "val: images/val\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "if classes is None:\n",
    "    raise RuntimeError(\n",
    "        \"Classes not defined. Please run the label conversion step first.\"\n",
    "    )\n",
    "\n",
    "config_path = FSOCO_YOLO / 'fsoco.yaml'\n",
    "\n",
    "if yolo_exists and config_path.exists():\n",
    "    print(f\"Config already exists: {config_path}\")\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(yaml.dump(config, default_flow_style=False))\n",
    "else:\n",
    "    config = {\n",
    "        'path': str(FSOCO_YOLO.absolute()),\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/val',\n",
    "        'test': 'images/test',\n",
    "        'nc': len(classes),\n",
    "        'names': classes\n",
    "    }\n",
    "    \n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "    \n",
    "    print(\"Config saved:\", config_path)\n",
    "    print(yaml.dump(config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. YOLOv8 Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: yolov8s.pt, Epochs: 200, Batch: -1, Image size: 640, Device: 0, Patience: 50\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "MODEL = 'yolov8n.pt'  # Options: yolov8n, yolov8s, yolov8m, yolov8l, yolov8x; change number to 10, 11\n",
    "EPOCHS = 200\n",
    "BATCH = -1  # auto batch size\n",
    "IMG_SIZE = 640\n",
    "DEVICE = \"0\" if torch.cuda.is_available() else \"cpu\"\n",
    "PROJECT = 'runs/detect'\n",
    "NAME = 'fsoco_train'\n",
    "PATIENCE = 50\n",
    "\n",
    "print(f\"Model: {MODEL}, Epochs: {EPOCHS}, Batch: {BATCH}, Image size: {IMG_SIZE}, Device: {DEVICE}, Patience: {PATIENCE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Updated 'tensorboard=True'\n",
      "JSONDict(\"/root/.config/Ultralytics/settings.json\"):\n",
      "{\n",
      "  \"settings_version\": \"0.0.6\",\n",
      "  \"datasets_dir\": \"/root/driverless-ml-dev/perceptions/camera_pipeline/notebooks/datasets\",\n",
      "  \"weights_dir\": \"weights\",\n",
      "  \"runs_dir\": \"runs\",\n",
      "  \"uuid\": \"fc99de1a4c779c5f163970f87d303313f87f7753e7e0d1d663314c6c845043d5\",\n",
      "  \"sync\": true,\n",
      "  \"api_key\": \"\",\n",
      "  \"openai_api_key\": \"\",\n",
      "  \"clearml\": true,\n",
      "  \"comet\": true,\n",
      "  \"dvc\": true,\n",
      "  \"hub\": true,\n",
      "  \"mlflow\": true,\n",
      "  \"neptune\": true,\n",
      "  \"raytune\": true,\n",
      "  \"tensorboard\": true,\n",
      "  \"wandb\": false,\n",
      "  \"vscode_msg\": true,\n",
      "  \"openvino_msg\": true\n",
      "}\n",
      "üí° Learn more about Ultralytics Settings at https://docs.ultralytics.com/quickstart/#ultralytics-settings\n"
     ]
    }
   ],
   "source": [
    "!yolo settings tensorboard=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Train YOLOv8 Model\n",
    "\n",
    "Using the Ultralytics CLI to train the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8s.pt to 'yolov8s.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 21.5MB 15.0MB/s 1.4s1.4s<0.0s\n",
      "New https://pypi.org/project/ultralytics/8.3.226 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.205 üöÄ Python-3.12.3 torch-2.8.0a0+34c6371d24.nv25.08 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=-1, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/root/driverless-ml-dev/ml_data/fsoco_yolo/fsoco.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=200, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8s.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=fsoco_train3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=50, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=runs/detect, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/root/driverless-ml-dev/perceptions/camera_pipeline/notebooks/runs/detect/fsoco_train3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /root/driverless-ml-dev/perceptions/camera_pipeline/notebooks/runs/detect/fsoco_train3', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=5\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
      "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
      "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
      "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
      "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
      " 22        [15, 18, 21]  1   2117983  ultralytics.nn.modules.head.Detect           [5, [128, 256, 512]]          \n",
      "Model summary: 129 layers, 11,137,535 parameters, 11,137,519 gradients, 28.7 GFLOPs\n",
      "\n",
      "Transferred 349/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5.4MB 19.8MB/s 0.3s.2s<0.1s\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 2.0¬±0.9 ms, read: 122.9¬±31.2 MB/s, size: 7453.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /root/driverless-ml-dev/ml_data/fsoco_yolo/labels/train... 5186 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5186/5186 35.5it/s 2:262.9ssss\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /root/driverless-ml-dev/ml_data/fsoco_yolo/labels/train.cache\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mComputing optimal batch size for imgsz=640 at 60.0% CUDA memory utilization.\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mCUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU) 8.00G total, 0.13G reserved, 0.12G allocated, 7.74G free\n",
      "      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output\n",
      "    11137535       28.66         0.805         63.58          1580        (1, 3, 640, 640)                    list\n",
      "    11137535       57.31         1.124         32.16         97.88        (2, 3, 640, 640)                    list\n",
      "    11137535       114.6         1.674         41.31           124        (4, 3, 640, 640)                    list\n",
      "    11137535       229.2         2.802         61.14         164.9        (8, 3, 640, 640)                    list\n",
      "    11137535       458.5         4.737         92.54         147.7       (16, 3, 640, 640)                    list\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mUsing batch-size 16 for CUDA:0 5.04G/8.00G (63%) ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 4.2¬±2.3 ms, read: 90.8¬±118.6 MB/s, size: 943.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /root/driverless-ml-dev/ml_data/fsoco_yolo/labels/train.cache... 5186 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5186/5186 54.9Mit/s 0.0s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 2.8¬±0.5 ms, read: 50.8¬±24.3 MB/s, size: 1812.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /root/driverless-ml-dev/ml_data/fsoco_yolo/labels/val... 236 images, 0 backgrounds, 0 corrupt: 36% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 236/648 52.0it/s 12.7s<7.9ss\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/data/dataset.py\", line 173, in get_labels\n",
      "    assert cache[\"version\"] == DATASET_CACHE_VERSION  # matches current version\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/pool.py\", line 856, in next\n",
      "    item = self._items.popleft()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/yolo\", line 7, in <module>\n",
      "    sys.exit(entrypoint())\n",
      "             ^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/cfg/__init__.py\", line 990, in entrypoint\n",
      "    getattr(model, mode)(**overrides)  # default args from model\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/model.py\", line 800, in train\n",
      "    self.trainer.train()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\", line 235, in train\n",
      "    self._do_train()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\", line 356, in _do_train\n",
      "    self._setup_train()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/engine/trainer.py\", line 320, in _setup_train\n",
      "    self.test_loader = self.get_dataloader(\n",
      "                       ^^^^^^^^^^^^^C\n",
      "^object address  : 0x7f5ddc3e5ba0\n",
      "object refcount : 3\n",
      "object type     : 0xa36620\n",
      "object type name: KeyboardInterrupt\n",
      "object repr     : KeyboardInterrupt()\n",
      "lost sys.stderr\n"
     ]
    }
   ],
   "source": [
    "!yolo detect train data={config_path} model={MODEL} epochs={EPOCHS} imgsz={IMG_SIZE} batch={BATCH} device={DEVICE} project={PROJECT} name={NAME} patience={PATIENCE} workers=0 rect=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IPImage\n",
    "from pathlib import Path\n",
    "\n",
    "runs_dir = Path('runs/detect')\n",
    "if runs_dir.exists():\n",
    "    runs = sorted(runs_dir.glob('fsoco_train*'), key=lambda x: x.stat().st_mtime)\n",
    "    if runs:\n",
    "        latest = runs[-1]\n",
    "        print(f\"Training run: {latest.name}\\n\")\n",
    "\n",
    "        results_img = latest / 'results.png'\n",
    "        if results_img.exists():\n",
    "            display(IPImage(filename=str(results_img)))\n",
    "        \n",
    "        print(f\"\\nWeights: {latest / 'weights' / 'best.pt'}\")\n",
    "\n",
    "        with mlflow.start_run(run_name=latest.name):\n",
    "            mlflow.log_params({\n",
    "                \"model\": MODEL,\n",
    "                \"epochs\": EPOCHS,\n",
    "                \"batch_size\": BATCH,\n",
    "                \"img_size\": IMG_SIZE,\n",
    "                \"device\": DEVICE\n",
    "            })\n",
    "            \n",
    "            results_csv = latest / 'results.csv'\n",
    "            if results_csv.exists():\n",
    "                import pandas as pd\n",
    "                df = pd.read_csv(results_csv)\n",
    "                if not df.empty:\n",
    "                    last_row = df.iloc[-1]\n",
    "                    for col in df.columns:\n",
    "                        if col.strip() and col.strip() != 'epoch':\n",
    "                            try:\n",
    "                                mlflow.log_metric(col.strip(), float(last_row[col]))\n",
    "                            except:\n",
    "                                pass\n",
    "        \n",
    "        mlflow.end_run()\n",
    "    else:\n",
    "        print(\"No training runs found\")\n",
    "else:\n",
    "    print(\"No runs directory found. Train the model first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to tensorrt\n",
    "from pathlib import Path\n",
    "\n",
    "runs_dir = Path('runs/detect')\n",
    "if runs_dir.exists():\n",
    "    runs = sorted(runs_dir.glob('fsoco_train*'), key=lambda x: x.stat().st_mtime)\n",
    "    if runs:\n",
    "        weight = runs[-1] / 'weights' / 'best.pt'\n",
    "        print(f\"Exporting weights: {weight}\")\n",
    "        !yolo export model={weight} format=engine device={DEVICE}\n",
    "    else:\n",
    "        print(\"No training runs found. Train the model first.\")\n",
    "else:\n",
    "    print(\"No runs directory found. Train the model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Export\n",
    "\n",
    "Export the trained model to formats:\n",
    "- `onnx`: ONNX format \n",
    "- `engine`: TensorRT (for jetson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "from pathlib import Path\n",
    "\n",
    "runs_dir = Path('runs/detect')\n",
    "if runs_dir.exists():\n",
    "    runs = sorted(runs_dir.glob('fsoco_train*'), key=lambda x: x.stat().st_mtime)\n",
    "    if runs:\n",
    "        weight = runs[-1] / 'weights' / 'best.pt'\n",
    "        print(f\"Exporting weights: {weight}\\n\")\n",
    "\n",
    "        formats = [\n",
    "            'onnx',      # ONNX format\n",
    "            # 'engine',  # TensorRT format\n",
    "        ]\n",
    "        \n",
    "        for fmt in formats:\n",
    "            print(f\"\\nExporting to {fmt}...\")\n",
    "            !yolo export model={weight} format={fmt} imgsz={IMG_SIZE}\n",
    "        \n",
    "        print(\"\\nExport complete!\")\n",
    "    else:\n",
    "        print(\"No training runs found. Train the model first.\")\n",
    "else:\n",
    "    print(\"No runs directory found. Train the model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "runs_dir = Path('runs/detect')\n",
    "if runs_dir.exists():\n",
    "    runs = sorted(runs_dir.glob('fsoco_train*'), key=lambda x: x.stat().st_mtime)\n",
    "    if runs:\n",
    "        weights = runs[-1] / 'weights' / 'best.pt'\n",
    "        test_imgs = FSOCO_YOLO / 'images' / 'test'\n",
    "        \n",
    "        if weights.exists() and test_imgs.exists():\n",
    "            print(\"Running inference with YOLOv8...\\n\")\n",
    "            #inference\n",
    "            !yolo detect predict model={weights} source={test_imgs} imgsz={IMG_SIZE} conf=0.25 save=True name=fsoco_inference project=runs/detect\n",
    "\n",
    "            # results\n",
    "            detect_runs = sorted(Path('runs/detect').glob('fsoco_inference*'), \n",
    "                               key=lambda x: x.stat().st_mtime)\n",
    "            if detect_runs:\n",
    "                print(f\"\\nResults saved to: {detect_runs[-1]}\")\n",
    "\n",
    "                results = list(detect_runs[-1].glob('*.jpg'))[:3]  # Show first 3 results\n",
    "                for r in results:\n",
    "                    print(f\"\\nShowing: {r.name}\")\n",
    "                    display(IPImage(filename=str(r), width=800))\n",
    "        else:\n",
    "            print(\"Missing weights or test images\")\n",
    "    else:\n",
    "        print(\"No training runs found. Train model first.\")\n",
    "else:\n",
    "    print(\"No runs directory found. Train model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Validate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "runs_dir = Path('runs/detect')\n",
    "if runs_dir.exists():\n",
    "    runs = sorted(runs_dir.glob('fsoco_train*'), key=lambda x: x.stat().st_mtime)\n",
    "    if runs:\n",
    "        weights = runs[-1] / 'weights' / 'best.pt'\n",
    "        \n",
    "        if weights.exists():\n",
    "            print(\"Running validation...\\n\")\n",
    "            \n",
    "            # validation\n",
    "            !yolo detect val model={weights} data={config_path} imgsz={IMG_SIZE} batch=1 device={DEVICE} save_json=True split=val\n",
    "            \n",
    "            print(\"\\nValidation complete!\")\n",
    "        else:\n",
    "            print(\"Weights not found\")\n",
    "    else:\n",
    "        print(\"No training runs found. Train model first.\")\n",
    "else:\n",
    "    print(\"No runs directory found. Train model first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmarking\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "runs_dir = Path('runs/detect')\n",
    "if runs_dir.exists():\n",
    "    candidates = sorted(runs_dir.glob(\"fsoco_train*/weights/best.pt\"), key=lambda p: p.stat().st_mtime)\n",
    "    \n",
    "    if candidates:\n",
    "        W = candidates[-1]\n",
    "        DATA_YAML = config_path\n",
    "        IMG = IMG_SIZE\n",
    "        DEVICE = \"0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        print(f\"Benchmarking weights: {W}\")\n",
    "        print(f\"Data: {DATA_YAML}\")\n",
    "        print(f\"Image size: {IMG}\")\n",
    "        print(f\"Device: {DEVICE}\\n\")\n",
    "        \n",
    "        # benchmark\n",
    "        !yolo detect val model={W} data={DATA_YAML} imgsz={IMG} batch=1 device={DEVICE} save_json=True\n",
    "        \n",
    "        # speed test\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"Running speed benchmark...\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        # speed\n",
    "        !yolo benchmark model={W} data={DATA_YAML} imgsz={IMG} device={DEVICE}\n",
    "        \n",
    "    else:\n",
    "        print(\"No trained weights found. Train the model first.\")\n",
    "else:\n",
    "    print(\"No runs directory found. Train the model first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
