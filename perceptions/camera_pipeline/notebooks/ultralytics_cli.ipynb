{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv5 FSOCO Training\n",
    "\n",
    "**Essential steps only:**\n",
    "1. Setup YOLOv5\n",
    "2. Convert FSOCO labels (Supervisely YOLO)\n",
    "3. Train\n",
    "4. Inference\n",
    "\n",
    "run this to include the yolov5 folder\n",
    "`git submodule update --init --recursive`\n",
    "**Run from:** `driverless-ml-dev/perceptions/camera-pipeline/notebooks/`\n",
    "\n",
    "**Activate venv in root/driverless-ml-dev directory, and connect kernel for this notebook to it**\n",
    "open vsc terminal via ctrl+shift+`\n",
    "\n",
    "first time only:\n",
    "```\n",
    "cd ~/driverless-ml-dev\n",
    "python -m venv venv\n",
    "source venv/bin/activate\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --name driverless-ml --display-name \"Python (driverless-ml)\" --user\n",
    "```\n",
    "then type `>developer reload` in the search bar at the top\n",
    "\n",
    "to activate venv\n",
    "```\n",
    "cd /root/driverless-ml-dev\n",
    "source venv/bin/activate\n",
    "```\n",
    "then type `>developer reload` in the search bar at the top\n",
    "\n",
    "install libraries with: `pip install pillow tqdm pyyaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to create and install a virtual environment\n",
    "# %cd ~/driverless-ml-dev\n",
    "# !python -m venv venv\n",
    "# !source venv/bin/activate\n",
    "# !venv/bin/python -m pip install --upgrade pip ipykernel\n",
    "# !venv/bin/python -m ipykernel install --name driverless-ml --display-name \"Python (driverless-ml)\" --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activate venv in root/driverless-ml-dev directory, and connect kernel for this notebook to it**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (6.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pillow tqdm pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/driverless-ml-dev\n",
      "Root: /root/driverless-ml-dev\n",
      "YOLOv5: /root/driverless-ml-dev/yolov5\n",
      "Data: /root/driverless-ml-dev/ml_data\n",
      "Fsoco_raw: /root/driverless-ml-dev/ml_data/fsoco_raw\n",
      "Fsoco_mod: /root/driverless-ml-dev/ml_data/fsoco_mod\n",
      "Fsoco_yolo: /root/driverless-ml-dev/ml_data/fsoco_yolo\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "\n",
    "# Paths\n",
    "ROOT = Path.cwd().parent.parent.parent  # driverless-ml-dev/\n",
    "print(ROOT)\n",
    "YOLO_DIR = ROOT / 'yolov5'\n",
    "DATA_DIR = ROOT / 'ml_data'\n",
    "FSOCO_RAW = ROOT / 'ml_data/fsoco_raw'  # download fsoco dataset\n",
    "FSOCO_MOD = ROOT / 'ml_data/fsoco_mod'  # copy of raw for preprocessing\n",
    "FSOCO_YOLO = ROOT / 'ml_data/fsoco_yolo'  # final dataset\n",
    "\n",
    "print(f\"Root: {ROOT}\")\n",
    "print(f\"YOLOv5: {YOLO_DIR}\")\n",
    "print(f\"Data: {DATA_DIR}\")\n",
    "print(f\"Fsoco_raw: {FSOCO_RAW}\")\n",
    "print(f\"Fsoco_mod: {FSOCO_MOD}\")\n",
    "print(f\"Fsoco_yolo: {FSOCO_YOLO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv5 exists\n",
      "Requirement already satisfied: gitpython>=3.1.30 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 5)) (3.1.45)\n",
      "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 6)) (3.10.5)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 7)) (1.26.4)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 8)) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 9)) (11.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 10)) (7.0.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 11)) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 12)) (2.32.4)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 13)) (1.15.3)\n",
      "Requirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 14)) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 15)) (2.8.0a0+34c6371d24.nv25.8)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 16)) (0.23.0a0+428a54c9)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 17)) (4.67.1)\n",
      "Requirement already satisfied: ultralytics>=8.2.64 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 18)) (8.3.205)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 27)) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 28)) (0.13.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 42)) (23.2)\n",
      "Requirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 43)) (79.0.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython>=3.1.30->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 5)) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 5)) (5.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 6)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 6)) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 6)) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 6)) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 12)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 12)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 12)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 12)) (2025.8.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 15)) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 15)) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 15)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 15)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 15)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 15)) (2025.7.0)\n",
      "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics>=8.2.64->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 18)) (1.28.1)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics>=8.2.64->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 18)) (2.0.17)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.4->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 27)) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.4->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 27)) (2025.2)\n",
      "Requirement already satisfied: gitpython>=3.1.30 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 5)) (3.1.45)\n",
      "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 6)) (3.10.5)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 7)) (1.26.4)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 8)) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 9)) (11.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 10)) (7.0.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 11)) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 12)) (2.32.4)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 13)) (1.15.3)\n",
      "Requirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 14)) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 15)) (2.8.0a0+34c6371d24.nv25.8)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 16)) (0.23.0a0+428a54c9)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 17)) (4.67.1)\n",
      "Requirement already satisfied: ultralytics>=8.2.64 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 18)) (8.3.205)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 27)) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 28)) (0.13.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 42)) (23.2)\n",
      "Requirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.12/dist-packages (from -r /root/driverless-ml-dev/yolov5/requirements.txt (line 43)) (79.0.1)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython>=3.1.30->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 5)) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 5)) (5.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 6)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 6)) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 6)) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 6)) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 12)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 12)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 12)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 12)) (2025.8.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 15)) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 15)) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 15)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 15)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 15)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 15)) (2025.7.0)\n",
      "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics>=8.2.64->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 18)) (1.28.1)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics>=8.2.64->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 18)) (2.0.17)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.4->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 27)) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.4->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 27)) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 15)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 15)) (3.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 15)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->-r /root/driverless-ml-dev/yolov5/requirements.txt (line 15)) (3.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Dependencies installed\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Dependencies installed\n"
     ]
    }
   ],
   "source": [
    "# Clone YOLOv5\n",
    "if not YOLO_DIR.exists() and not any(YOLO_DIR.iterdir()):\n",
    "    !cd {ROOT} && git clone https://github.com/ultralytics/yolov5.git\n",
    "    print(\"YOLOv5 cloned\")\n",
    "else:\n",
    "    print(\"YOLOv5 exists\")\n",
    "\n",
    "# Install requirements\n",
    "%pip install -r {YOLO_DIR}/requirements.txt\n",
    "print(\"Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download FSOCO Dataset\n",
    "\n",
    "**Manual step required:**\n",
    "1. Visit: https://fsoco.github.io/fsoco-dataset/download\n",
    "2. Download Bounding Boxes dataset (24GB)\n",
    "3. Extract to: `root/ml-data/perceptions/`\n",
    "\n",
    "Expected structure:\n",
    "```\n",
    "root/ml-data/perceptions/\n",
    "└── fsoco_raw/\n",
    "    ├── team1/ann/       # JSON annotations\n",
    "    ├── team1/img/       # Images\n",
    "    └── meta.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "/root/driverless-ml-dev/ml_data/fsoco_raw\n",
      "True\n",
      "True\n",
      "FSOCO dataset found\n",
      "Skipping flatten: working copy already populated /root/driverless-ml-dev/ml_data/fsoco_mod\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def copy_and_flatten_dataset(source_path: Path, dest_path: Path):\n",
    "    \"\"\"\n",
    "    flattening raw dataset:\n",
    "    source/\n",
    "      - meta.json\n",
    "      - teamA/{ann,img}\n",
    "      - teamB/{ann,img}\n",
    "      ...\n",
    "    ->\n",
    "    dest/\n",
    "      - ann/\n",
    "      - img/\n",
    "      - meta.json\n",
    "    \"\"\"\n",
    "    if not source_path.exists() or not any(source_path.iterdir()):\n",
    "        raise FileNotFoundError(f\"Source dataset not found or empty: {source_path}\")\n",
    "\n",
    "    if dest_path.exists() and any(dest_path.iterdir()):\n",
    "        print(f\"Skipping copy: destination already populated {dest_path}\")\n",
    "        return\n",
    "\n",
    "    dest_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for item in dest_path.iterdir():\n",
    "        if item.is_dir():\n",
    "            shutil.rmtree(item)\n",
    "        else:\n",
    "            item.unlink()\n",
    "\n",
    "    ann_out = dest_path / \"ann\"\n",
    "    img_out = dest_path / \"img\"\n",
    "    ann_out.mkdir(exist_ok=True)\n",
    "    img_out.mkdir(exist_ok=True)\n",
    "\n",
    "    meta_src = source_path / \"meta.json\"\n",
    "    if meta_src.exists():\n",
    "        shutil.copy2(meta_src, dest_path / \"meta.json\")\n",
    "\n",
    "    ann_count = 0\n",
    "    img_count = 0\n",
    "\n",
    "    for team_dir in source_path.iterdir():\n",
    "        print(f\"Processing team directory: {team_dir.name}\")\n",
    "        if not team_dir.is_dir():\n",
    "            print(f\"Skipping non-directory item: {team_dir.name}\")\n",
    "            continue\n",
    "\n",
    "        if team_dir.name in {\"ann\", \"img\"}:\n",
    "            continue\n",
    "\n",
    "        ann_dir = team_dir / \"ann\"\n",
    "        img_dir = team_dir / \"img\"\n",
    "\n",
    "        if ann_dir.exists():\n",
    "            for f in ann_dir.iterdir():\n",
    "                if f.is_file():\n",
    "                    dest = ann_out / f\"{team_dir.name}_{f.name}\"\n",
    "                    shutil.copy2(f, dest)\n",
    "                    ann_count += 1\n",
    "\n",
    "        if img_dir.exists():\n",
    "            for f in img_dir.iterdir():\n",
    "                if f.is_file():\n",
    "                    dest = img_out / f\"{team_dir.name}_{f.name}\"\n",
    "                    shutil.copy2(f, dest)\n",
    "                    img_count += 1\n",
    "\n",
    "    print(f\"Copy+flatten complete {ann_out} ({ann_count} files), {img_out} ({img_count} files)\")\n",
    "\n",
    "\n",
    "raw_exists = FSOCO_RAW.exists()\n",
    "raw_has_content = os.path.isdir(FSOCO_RAW) if raw_exists else False\n",
    "\n",
    "# used later on to skip ratio filtering since it assumes you already ran it\n",
    "mod_exists = FSOCO_MOD.exists()\n",
    "if raw_exists and raw_has_content:\n",
    "    print(\"FSOCO dataset found\")\n",
    "    if FSOCO_MOD.exists() and any(FSOCO_MOD.iterdir()):\n",
    "        print(f\"Skipping flatten: working copy already populated {FSOCO_MOD}\")\n",
    "    else:\n",
    "        for item in FSOCO_RAW.iterdir():\n",
    "            print(f\"  - {item.name}\")\n",
    "        FSOCO_MOD.mkdir(parents=True, exist_ok=True)\n",
    "        copy_and_flatten_dataset(FSOCO_RAW, FSOCO_MOD)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Download FSOCO dataset to: {FSOCO_RAW}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting dataset at: /root/driverless-ml-dev/ml_data/fsoco_mod\n",
      "/root/driverless-ml-dev/ml_data/fsoco_mod/ann\n",
      "ann contents:\n",
      "  amz_amz_00000.jpg.json\n",
      "  amz_amz_00001.jpg.json\n",
      "  amz_amz_00002.jpg.json\n",
      "  amz_amz_00003.jpg.json\n",
      "  amz_amz_00004.jpg.json\n",
      "\n",
      "img contents:\n",
      "  amz_amz_00000.jpg\n",
      "  amz_amz_00001.jpg\n",
      "  amz_amz_00002.jpg\n",
      "  amz_amz_00003.jpg\n",
      "  amz_amz_00004.jpg\n",
      "\n",
      "meta.json:\n",
      "{\n",
      "  \"classes\": [\n",
      "    {\n",
      "      \"title\": \"seg_orange_cone\",\n",
      "      \"shape\": \"bitmap\",\n",
      "      \"color\": \"#FF8000\",\n",
      "      \"geometry_config\": {},\n",
      "      \"id\": 9993505,\n",
      "      \"hotkey\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"unknown_cone\",\n",
      "      \"shape\": \"rectangle\",\n",
      "      \"color\": \"#3BDB0F\",\n",
      "      \"geometry_config\": {},\n",
      "      \"id\": 9993514,\n",
      "      \"hotkey\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"yellow_cone\",\n",
      "      \"shape\": \"rectangle\",\n",
      "      \"color\": \"#FFFF00\",\n",
      "      \"geometry_config\": {},\n",
      "      \"id\": 9993506,\n",
      "      \"ho\n",
      "  amz_amz_00000.jpg\n",
      "  amz_amz_00001.jpg\n",
      "  amz_amz_00002.jpg\n",
      "  amz_amz_00003.jpg\n",
      "  amz_amz_00004.jpg\n",
      "\n",
      "meta.json:\n",
      "{\n",
      "  \"classes\": [\n",
      "    {\n",
      "      \"title\": \"seg_orange_cone\",\n",
      "      \"shape\": \"bitmap\",\n",
      "      \"color\": \"#FF8000\",\n",
      "      \"geometry_config\": {},\n",
      "      \"id\": 9993505,\n",
      "      \"hotkey\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"unknown_cone\",\n",
      "      \"shape\": \"rectangle\",\n",
      "      \"color\": \"#3BDB0F\",\n",
      "      \"geometry_config\": {},\n",
      "      \"id\": 9993514,\n",
      "      \"hotkey\": \"\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"yellow_cone\",\n",
      "      \"shape\": \"rectangle\",\n",
      "      \"color\": \"#FFFF00\",\n",
      "      \"geometry_config\": {},\n",
      "      \"id\": 9993506,\n",
      "      \"ho\n"
     ]
    }
   ],
   "source": [
    "if not FSOCO_MOD.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Working dataset directory not found: {FSOCO_MOD}. Run the preparation step and ensure the directory exists.\"\n",
    "    )\n",
    "\n",
    "if not any(FSOCO_MOD.iterdir()):\n",
    "    raise RuntimeError(\n",
    "        f\"Working dataset directory is empty: {FSOCO_MOD}. Run the preparation step to copy data before proceeding.\"\n",
    "    )\n",
    "\n",
    "working_dataset = FSOCO_MOD\n",
    "print(f\"Inspecting dataset at: {working_dataset}\")\n",
    "\n",
    "bb_path = working_dataset / 'ann'\n",
    "print(bb_path)\n",
    "print(\"ann contents:\")\n",
    "if bb_path.exists():\n",
    "    items = list(os.listdir(bb_path))[:5]\n",
    "    for item in items:\n",
    "        print(f\"  {item}\")\n",
    "else:\n",
    "    print(\"no ann directory found\")\n",
    "\n",
    "img_path = working_dataset / 'img'\n",
    "\n",
    "print(\"\\nimg contents:\")\n",
    "if img_path.exists():\n",
    "    items = list(os.listdir(img_path))[:5]\n",
    "    for item in items:\n",
    "        print(f\"  {item}\")\n",
    "else:\n",
    "    print(\"no image directory found\")\n",
    "\n",
    "print(\"\\nmeta.json:\")\n",
    "meta_path = working_dataset / 'meta.json'\n",
    "\n",
    "if meta_path.exists():\n",
    "    import json\n",
    "    meta = json.load(open(meta_path))\n",
    "    print(json.dumps(meta, indent=2)[:500])\n",
    "else:\n",
    "    print(\"meta.json not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filter Images by Aspect Ratio\n",
    "\n",
    "Remove images that don't meet minimum aspect ratio requirements (width/height)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping aspect ratio filtering: already applied previously.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "def filter_images_by_aspect_ratio(fsoco_mod, min_ratio=1.0, max_ratio=3.0):\n",
    "    img_dir = fsoco_mod / 'images'\n",
    "    if not img_dir.exists():\n",
    "        img_dir = fsoco_mod / 'img'\n",
    "    ann_dir = fsoco_mod / 'bounding_boxes'\n",
    "    if not ann_dir.exists():\n",
    "        ann_dir = fsoco_mod / 'ann'\n",
    "    \n",
    "    if not img_dir.exists():\n",
    "        print(f\"Image directory not found: {img_dir}\")\n",
    "        return [], 0\n",
    "    \n",
    "    images = list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png'))\n",
    "    print(f\"Total images found: {len(images)}\")\n",
    "    \n",
    "    filtered_images = []\n",
    "    removed_images = []\n",
    "    kept_dimensions = []\n",
    "    removed_dimensions = []\n",
    "    kept_aspect_ratios = []\n",
    "    removed_aspect_ratios = []\n",
    "    \n",
    "    for img_path in tqdm(images, desc=\"Filtering images by aspect ratio\"):\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            w, h = img.size\n",
    "            aspect_ratio = w / h\n",
    "            \n",
    "            if min_ratio <= aspect_ratio <= max_ratio:\n",
    "                filtered_images.append(img_path)\n",
    "                kept_dimensions.append((w, h))\n",
    "                kept_aspect_ratios.append(aspect_ratio)\n",
    "            else:\n",
    "                removed_images.append((img_path, aspect_ratio, w, h))\n",
    "                removed_dimensions.append((w, h))\n",
    "                removed_aspect_ratios.append(aspect_ratio)\n",
    "                \n",
    "                img_path.unlink()\n",
    "                \n",
    "                ann_path = ann_dir / f\"{img_path.name}.json\"\n",
    "                if not ann_path.exists():\n",
    "                    ann_path = ann_dir / f\"{img_path.stem}.json\"\n",
    "                if ann_path.exists():\n",
    "                    ann_path.unlink()\n",
    "                \n",
    "                print(f\"Removed: {img_path.name} (aspect ratio: {aspect_ratio:.2f}, dims: {w}x{h})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path.name}: {e}\")\n",
    "            continue\n",
    "    #- Print detailed statistics -\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FILTERING STATISTICS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"\\nOVERALL SUMMARY:\")\n",
    "    print(f\"  Total images processed:  {len(images)}\")\n",
    "    if images:\n",
    "        print(f\"  Images kept:           {len(filtered_images)} ({100*len(filtered_images)/len(images):.1f}%)\")\n",
    "        print(f\"  Images filtered out:   {len(removed_images)} ({100*len(removed_images)/len(images):.1f}%)\")\n",
    "    else:\n",
    "        print(\"  Images kept:           0\")\n",
    "        print(\"  Images filtered out:   0\")\n",
    "    print(f\"  Aspect ratio range:      {min_ratio} - {max_ratio}\")\n",
    "    \n",
    "    if kept_aspect_ratios:\n",
    "        print(f\"\\nKEPT IMAGES - ASPECT RATIO STATS:\")\n",
    "        print(f\"  Min aspect ratio:  {min(kept_aspect_ratios):.2f}\")\n",
    "        print(f\"  Max aspect ratio:  {max(kept_aspect_ratios):.2f}\")\n",
    "        print(f\"  Avg aspect ratio:  {sum(kept_aspect_ratios)/len(kept_aspect_ratios):.2f}\")\n",
    "    \n",
    "    if kept_dimensions:\n",
    "        print(f\"\\nKEPT IMAGES - DIMENSION STATS:\")\n",
    "        dim_counter = Counter(kept_dimensions)\n",
    "        top_dims = dim_counter.most_common(10)\n",
    "        print(f\"  Unique dimensions: {len(dim_counter)}\")\n",
    "        print(f\"  Top 10 dimensions:\")\n",
    "        for (w, h), count in top_dims:\n",
    "            ratio = w/h\n",
    "            print(f\"    {w}x{h} (ratio {ratio:.2f}): {count} images\")\n",
    "    \n",
    "    if removed_aspect_ratios:\n",
    "        print(f\"\\nREMOVED IMAGES - ASPECT RATIO STATS:\")\n",
    "        print(f\"  Min aspect ratio:  {min(removed_aspect_ratios):.2f}\")\n",
    "        print(f\"  Max aspect ratio:  {max(removed_aspect_ratios):.2f}\")\n",
    "        print(f\"  Avg aspect ratio:  {sum(removed_aspect_ratios)/len(removed_aspect_ratios):.2f}\")\n",
    "    \n",
    "    if removed_dimensions:\n",
    "        print(f\"\\nREMOVED IMAGES - DIMENSION STATS:\")\n",
    "        dim_counter = Counter(removed_dimensions)\n",
    "        print(f\"  Unique dimensions: {len(dim_counter)}\")\n",
    "        print(f\"  All removed dimensions:\")\n",
    "        for (w, h), count in sorted(dim_counter.items(), key=lambda x: x[1], reverse=True):\n",
    "            ratio = w/h\n",
    "            print(f\"    {w}x{h} (ratio {ratio:.2f}): {count} images\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "    \n",
    "    return filtered_images, len(removed_images)\n",
    "\n",
    "MIN_ASPECT_RATIO = 1.25  # minimum width/height ratio\n",
    "MAX_ASPECT_RATIO = 1.80  # maximum width/height ratio\n",
    "\n",
    "if not FSOCO_MOD.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Working dataset directory not found: {FSOCO_MOD}. Create it and rerun the preparation step before filtering.\"\n",
    "    )\n",
    "\n",
    "if not any(FSOCO_MOD.iterdir()):\n",
    "    raise RuntimeError(\n",
    "        f\"Working dataset directory is empty: {FSOCO_MOD}. Run the preparation step to copy data before filtering.\"\n",
    "    )\n",
    "\n",
    "if not mod_exists:\n",
    "    print(f\"Using working dataset: {FSOCO_MOD}\")\n",
    "    filtered_imgs, removed = filter_images_by_aspect_ratio(\n",
    "        FSOCO_MOD,\n",
    "        min_ratio=MIN_ASPECT_RATIO,\n",
    "        max_ratio=MAX_ASPECT_RATIO\n",
    "    )\n",
    "    mod_exists = True\n",
    "    print(\"Aspect ratio filtering complete. Flag updated to skip on next run.\")\n",
    "else:\n",
    "    print(\"Skipping aspect ratio filtering: already applied previously.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convert Labels: Supervisely YOLO\n",
    "\n",
    "YOLO format: `<class_id> <x_center> <y_center> <width> <height>` (all normalized 0-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 classes from meta.json: ['unknown_cone', 'yellow_cone', 'blue_cone', 'orange_cone', 'large_orange_cone']\n",
      "Skipping conversion: YOLO dataset already populated /root/driverless-ml-dev/ml_data/fsoco_yolo\n",
      "\n",
      "Final classes list (5 classes): ['unknown_cone', 'yellow_cone', 'blue_cone', 'orange_cone', 'large_orange_cone']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "def convert_supervisely_to_yolo(fsoco_raw, fsoco_yolo, split=(0.8, 0.1, 0.1)):\n",
    "    for s in ['train', 'val', 'test']:\n",
    "        (fsoco_yolo / 'images' / s).mkdir(parents=True, exist_ok=True)\n",
    "        (fsoco_yolo / 'labels' / s).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # hard code class names\n",
    "    classes = ['blue_cone', 'unknown_cone', 'orange_cone', 'large_orange_cone', 'yellow_cone']\n",
    "    class_map = {name: i for i, name in enumerate(classes)}\n",
    "    print(f\"Classes (bounding boxes only): {classes}\\n\")\n",
    "    print(f\"Class mapping: {class_map}\\n\")\n",
    "\n",
    "    ann_dir = fsoco_raw / 'ann'\n",
    "    img_dir = fsoco_raw / 'img'\n",
    "    \n",
    "    ann_files = list(ann_dir.glob('*.json'))\n",
    "    print(f\"Total annotations: {len(ann_files)}\")\n",
    "    \n",
    "    random.shuffle(ann_files)\n",
    "    n1 = int(len(ann_files) * split[0])\n",
    "    n2 = int(len(ann_files) * (split[0] + split[1]))\n",
    "    splits = {\n",
    "        'train': ann_files[:n1],\n",
    "        'val': ann_files[n1:n2],\n",
    "        'test': ann_files[n2:]\n",
    "    }\n",
    "    \n",
    "    for split_name, anns in splits.items():\n",
    "        print(f\"\\nConverting {split_name}: {len(anns)} images\")\n",
    "        \n",
    "        for ann_path in tqdm(anns):\n",
    "            img_name = ann_path.stem\n",
    "            img_path = img_dir / img_name\n",
    "            \n",
    "            if not img_path.exists():\n",
    "                alt_name = img_name.replace('.jpg', '.png') if '.jpg' in img_name else img_name.replace('.png', '.jpg')\n",
    "                img_path = img_dir / alt_name\n",
    "                if not img_path.exists():\n",
    "                    continue\n",
    "\n",
    "            img = Image.open(img_path)\n",
    "            w, h = img.size\n",
    "\n",
    "            ann = json.load(open(ann_path))\n",
    "\n",
    "            yolo_labels = []\n",
    "            for obj in ann.get('objects', []):\n",
    "                cls = obj['classTitle']\n",
    "                if cls not in class_map or obj['geometryType'] != 'rectangle':\n",
    "                    continue\n",
    "                \n",
    "                points = obj['points']['exterior']\n",
    "                x1, y1 = points[0]\n",
    "                x2, y2 = points[1]\n",
    "\n",
    "                x_center = ((x1 + x2) / 2) / w\n",
    "                y_center = ((y1 + y2) / 2) / h\n",
    "                width = abs(x2 - x1) / w\n",
    "                height = abs(y2 - y1) / h\n",
    "\n",
    "                x_center = max(0, min(1, x_center))\n",
    "                y_center = max(0, min(1, y_center))\n",
    "                width = max(0, min(1, width))\n",
    "                height = max(0, min(1, height))\n",
    "                \n",
    "                yolo_labels.append(f\"{class_map[cls]} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\")\n",
    "            \n",
    "            if yolo_labels:\n",
    "                shutil.copy(img_path, fsoco_yolo / 'images' / split_name / img_path.name)\n",
    "                with open(fsoco_yolo / 'labels' / split_name / f\"{img_path.stem}.txt\", 'w') as f:\n",
    "                    f.write('\\n'.join(yolo_labels))\n",
    "    \n",
    "    print(\"\\nConversion complete\")\n",
    "    print(f\"\\nDataset splits:\")\n",
    "    for split_name in ['train', 'val', 'test']:\n",
    "        n_imgs = len(list((fsoco_yolo / 'images' / split_name).glob('*.[jp][pn]g')))\n",
    "        print(f\"  {split_name}: {n_imgs} images\")\n",
    "    \n",
    "    return classes\n",
    "\n",
    "classes = None\n",
    "if (FSOCO_MOD / 'meta.json').exists():\n",
    "    try:\n",
    "        meta = json.load(open(FSOCO_MOD / 'meta.json'))\n",
    "        classes = [c['title'] for c in meta['classes'] if c['shape'] == 'rectangle']\n",
    "        print(f\"Loaded {len(classes)} classes from meta.json: {classes}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to load classes from meta.json: {e}\")\n",
    "\n",
    "yolo_exists = FSOCO_YOLO.exists() and any(FSOCO_YOLO.iterdir())\n",
    "\n",
    "if not (FSOCO_MOD.exists() and any(FSOCO_MOD.iterdir())):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Working dataset not found or empty: {FSOCO_MOD}\\n\"\n",
    "        f\"Please run the dataset preparation step first (cell 10).\"\n",
    "    )\n",
    "\n",
    "if yolo_exists:\n",
    "    print(f\"Skipping conversion: YOLO dataset already populated {FSOCO_YOLO}\")\n",
    "else:\n",
    "    print(f\"Using working dataset: {FSOCO_MOD}\")\n",
    "    classes = convert_supervisely_to_yolo(FSOCO_MOD, FSOCO_YOLO)\n",
    "\n",
    "if classes is None:\n",
    "    raise RuntimeError(\n",
    "        \"Failed to load class definitions.\\n\"\n",
    "        f\"Ensure meta.json exists in {FSOCO_MOD} or run the conversion.\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nFinal classes list ({len(classes)} classes): {classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Dataset Config (YAML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config already exists: /root/driverless-ml-dev/ml_data/fsoco_yolo/fsoco.yaml\n",
      "names:\n",
      "- unknown_cone\n",
      "- yellow_cone\n",
      "- blue_cone\n",
      "- orange_cone\n",
      "- large_orange_cone\n",
      "nc: 5\n",
      "path: /root/driverless-ml-dev/ml_data/perceptions/fsoco_yolo\n",
      "test: images/test\n",
      "train: images/train\n",
      "val: images/val\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "if classes is None:\n",
    "    raise RuntimeError(\n",
    "        \"Classes not defined. Please run the label conversion step first.\"\n",
    "    )\n",
    "\n",
    "config_path = FSOCO_YOLO / 'fsoco.yaml'\n",
    "\n",
    "if yolo_exists and config_path.exists():\n",
    "    print(f\"Config already exists: {config_path}\")\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(yaml.dump(config, default_flow_style=False))\n",
    "else:\n",
    "    config = {\n",
    "        'path': str(FSOCO_YOLO.absolute()),\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/val',\n",
    "        'test': 'images/test',\n",
    "        'nc': len(classes),\n",
    "        'names': classes\n",
    "    }\n",
    "    \n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "    \n",
    "    print(\"Config saved:\", config_path)\n",
    "    print(yaml.dump(config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train YOLOv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: yolov5s.pt, Epochs: 100, Batch: -1, Image size: 640\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "import torch\n",
    "MODEL = 'yolov5s.pt'  # Options: yolov5n, yolov5s, yolov5m, yolov5l, yolov5x\n",
    "EPOCHS = 100\n",
    "BATCH = -1\n",
    "IMG_SIZE = 640\n",
    "DEVICE = \"0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Model: {MODEL}, Epochs: {EPOCHS}, Batch: {BATCH}, Image size: {IMG_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=/root/driverless-ml-dev/ml_data/fsoco_yolo/fsoco.yaml, hyp=../../../yolov5/data/hyps/hyp.scratch-low.yaml, epochs=100, batch_size=-1, imgsz=640, rect=True, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=../../../yolov5/data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=0, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=0, project=../../../yolov5/runs/train, name=fsoco_yolov5s.pt, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0m⚠️ YOLOv5 is out of date by 10 commits. Use 'git pull' or 'git clone https://github.com/ultralytics/yolov5' to update.\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0m⚠️ YOLOv5 is out of date by 10 commits. Use 'git pull' or 'git clone https://github.com/ultralytics/yolov5' to update.\n",
      "YOLOv5 🚀 2025-10-10 Python-3.12.3 torch-2.8.0a0+34c6371d24.nv25.08 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir ../../../yolov5/runs/train', view at http://localhost:6006/\n",
      "YOLOv5 🚀 2025-10-10 Python-3.12.3 torch-2.8.0a0+34c6371d24.nv25.08 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir ../../../yolov5/runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=5\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "Overriding model.yaml nc=80 with nc=5\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     26970  models.yolo.Detect                      [5, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     26970  models.yolo.Detect                      [5, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7033114 parameters, 7033114 gradients, 16.0 GFLOPs\n",
      "\n",
      "Model summary: 214 layers, 7033114 parameters, 7033114 gradients, 16.0 GFLOPs\n",
      "\n",
      "Transferred 343/349 items from yolov5s.pt\n",
      "Transferred 343/349 items from yolov5s.pt\n",
      "/root/driverless-ml-dev/yolov5/models/common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "/root/driverless-ml-dev/yolov5/models/common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "/root/driverless-ml-dev/yolov5/models/common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "/root/driverless-ml-dev/yolov5/models/common.py:906: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "/root/driverless-ml-dev/yolov5/utils/autobatch.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(amp):\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "/root/driverless-ml-dev/yolov5/utils/autobatch.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(amp):\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mComputing optimal batch size for --imgsz 640\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mCUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU) 8.00G total, 0.10G reserved, 0.05G allocated, 7.85G free\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mComputing optimal batch size for --imgsz 640\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mCUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU) 8.00G total, 0.10G reserved, 0.05G allocated, 7.85G free\n",
      "      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output\n",
      "      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output\n",
      "     7033114       15.98         0.256         46.56         146.7        (1, 3, 640, 640)                    list\n",
      "     7033114       15.98         0.256         46.56         146.7        (1, 3, 640, 640)                    list\n",
      "     7033114       31.96         0.445         17.74         21.17        (2, 3, 640, 640)                    list\n",
      "     7033114       31.96         0.445         17.74         21.17        (2, 3, 640, 640)                    list\n",
      "     7033114       63.92         0.902         18.98         26.82        (4, 3, 640, 640)                    list\n",
      "     7033114       63.92         0.902         18.98         26.82        (4, 3, 640, 640)                    list\n",
      "     7033114       127.8         1.648         25.35         34.42        (8, 3, 640, 640)                    list\n",
      "     7033114       127.8         1.648         25.35         34.42        (8, 3, 640, 640)                    list\n",
      "     7033114       255.7         3.173         52.41         61.12       (16, 3, 640, 640)                    list\n",
      "     7033114       255.7         3.173         52.41         61.12       (16, 3, 640, 640)                    list\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mUsing batch-size 31 for CUDA:0 6.25G/8.00G (78%) ✅\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.000484375), 60 bias\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mUsing batch-size 31 for CUDA:0 6.25G/8.00G (78%) ✅\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.000484375), 60 bias\n",
      "WARNING ⚠️ --rect is incompatible with DataLoader shuffle, setting shuffle=False\n",
      "WARNING ⚠️ --rect is incompatible with DataLoader shuffle, setting shuffle=False\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /root/driverless-ml-dev/ml_data/fsoco_yolo/labels/train... 1735 \u001b[0mProcess ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-5:\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /root/driverless-ml-dev/ml_data/fsoco_yolo/labels/train... 1735 \u001b[0m\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/driverless-ml-dev/yolov5/utils/dataloaders.py\", line 600, in __init__\n",
      "    assert cache[\"hash\"] == get_hash(self.label_files + self.im_files)  # identical hash\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/pool.py\", line 856, in next\n",
      "    item = self._items.popleft()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/driverless-ml-dev/yolov5/train.py\", line 988, in <module>\n",
      "    main(opt)\n",
      "  File \"/root/driverless-ml-dev/yolov5/train.py\", line 690, in main\n",
      "    train(opt.hyp, opt, device, callbacks)\n",
      "  File \"/root/driverless-ml-dev/yolov5/train.py\", line 287, in train\n",
      "    train_loader, dataset = create_dataloader(\n",
      "                            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/driverless-ml-dev/yolov5/utils/dataloaders.py\", line 184, in create_dataloader\n",
      "    dataset = LoadImagesAndLabels(\n",
      "              ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/driverless-ml-dev/yolov5/utils/dataloaders.py\", line 602, in __init__\n",
      "    cache, exists = self.cache_labels(cache_path, prefix), False  # run cache ops\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/driverless-ml-dev/yolov5/utils/dataloaders.py\", line 734, in cache_labels\n",
      "    for im_file, lb, shape, segments, nm_f, nf_f, ne_f, nc_f, msg in pbar:\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tqdm/std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/usr/lib/python3.12/multiprocessing/pool.py\", line 861, in next\n",
      "    self._cond.wait(timeout)\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 355, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "^C\n",
      "Process ForkPoolWorker-4:\n",
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-5:\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /root/driverless-ml-dev/ml_data/fsoco_yolo/labels/train... 1735 \u001b[0m\n",
      "Process ForkPoolWorker-7:\n",
      "Process ForkPoolWorker-1:\n",
      "Process ForkPoolWorker-2:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/driverless-ml-dev/yolov5/utils/dataloaders.py\", line 600, in __init__\n",
      "    assert cache[\"hash\"] == get_hash(self.label_files + self.im_files)  # identical hash\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.12/multiprocessing/pool.py\", line 856, in next\n",
      "    item = self._items.popleft()\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/driverless-ml-dev/yolov5/train.py\", line 988, in <module>\n",
      "    main(opt)\n",
      "  File \"/root/driverless-ml-dev/yolov5/train.py\", line 690, in main\n",
      "    train(opt.hyp, opt, device, callbacks)\n",
      "  File \"/root/driverless-ml-dev/yolov5/train.py\", line 287, in train\n",
      "    train_loader, dataset = create_dataloader(\n",
      "                            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/driverless-ml-dev/yolov5/utils/dataloaders.py\", line 184, in create_dataloader\n",
      "    dataset = LoadImagesAndLabels(\n",
      "              ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/driverless-ml-dev/yolov5/utils/dataloaders.py\", line 602, in __init__\n",
      "    cache, exists = self.cache_labels(cache_path, prefix), False  # run cache ops\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/driverless-ml-dev/yolov5/utils/dataloaders.py\", line 734, in cache_labels\n",
      "    for im_file, lb, shape, segments, nm_f, nf_f, ne_f, nc_f, msg in pbar:\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/tqdm/std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/usr/lib/python3.12/multiprocessing/pool.py\", line 861, in next\n",
      "    self._cond.wait(timeout)\n",
      "  File \"/usr/lib/python3.12/threading.py\", line 355, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "!python {YOLO_DIR}/train.py \\\n",
    "  --imgsz {IMG_SIZE} \\\n",
    "  --batch-size {BATCH} \\\n",
    "  --epochs {EPOCHS} \\\n",
    "  --data {config_path} \\\n",
    "  --weights {MODEL} \\\n",
    "  --name fsoco_{MODEL} \\\n",
    "  --rect \\\n",
    "  --device {DEVICE} \\\n",
    "  --workers 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training run: fsoco_yolov5s.pt2\n",
      "\n",
      "\n",
      "Weights: /root/driverless-ml-dev/yolov5/runs/train/fsoco_yolov5s.pt2/weights/best.pt\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image as IPImage\n",
    "\n",
    "runs = sorted((YOLO_DIR / 'runs' / 'train').glob('fsoco_*'), key=lambda x: x.stat().st_mtime)\n",
    "if runs:\n",
    "    latest = runs[-1]\n",
    "    print(f\"Training run: {latest.name}\\n\")\n",
    "\n",
    "    results_img = latest / 'results.png'\n",
    "    if results_img.exists():\n",
    "        display(IPImage(filename=str(results_img)))\n",
    "    \n",
    "    print(f\"\\nWeights: {latest / 'weights' / 'best.pt'}\")\n",
    "else:\n",
    "    print(\"No training runs found\")\n",
    "    \n",
    "with mlflow.start_run(run_name=\"test\"):\n",
    "    mlflow.log_params({\n",
    "        \"model\": MODEL,\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH,\n",
    "        \"img_size\": IMG_SIZE,\n",
    "    })\n",
    "    mlflow.log_metric(\"accuracy\", 0.95, step=1)\n",
    "    #also want info abt dataset\n",
    "\n",
    "mlflow.end_run()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing weights or test images\n"
     ]
    }
   ],
   "source": [
    "if runs:\n",
    "    weights = runs[-1] / 'weights' / 'best.pt'\n",
    "    test_imgs = FSOCO_YOLO / 'images' / 'test'\n",
    "    \n",
    "    if weights.exists() and test_imgs.exists():\n",
    "        print(\"Running inference...\\n\")\n",
    "        \n",
    "        !python {YOLO_DIR}/detect.py \\\n",
    "            --weights {weights} \\\n",
    "            --source {test_imgs} \\\n",
    "            --img {IMG_SIZE} \\\n",
    "            --conf 0.25 \\\n",
    "            --name fsoco_inference \\\n",
    "            --max-det 100\n",
    "\n",
    "        detect_runs = sorted((YOLO_DIR / 'runs' / 'detect').glob('fsoco_inference*'), \n",
    "                           key=lambda x: x.stat().st_mtime)\n",
    "        if detect_runs:\n",
    "            print(f\"\\nResults saved to: {detect_runs[-1]}\")\n",
    "\n",
    "            results = list(detect_runs[-1].glob('*.jpg'))[:1]\n",
    "            for r in results:\n",
    "                display(IPImage(filename=str(r), width=800))\n",
    "    else:\n",
    "        print(\"Missing weights or test images\")\n",
    "else:\n",
    "    print(\"Train model first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using weights: /root/driverless-ml-dev/yolov5/runs/train/fsoco_yolov5n3/weights/best.pt\n",
      "Data: /root/driverless-ml-dev/ml_data/fsoco_yolo/fsoco.yaml\n",
      "img: 640  device: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 🚀 2025-10-10 Python-3.12.3 torch-2.8.0a0+34c6371d24.nv25.08 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\n",
      "YOLOv5 🚀 2025-10-10 Python-3.12.3 torch-2.8.0a0+34c6371d24.nv25.08 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\n",
      "YOLOv5 🚀 2025-10-10 Python-3.12.3 torch-2.8.0a0+34c6371d24.nv25.08 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 8188MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 1765930 parameters, 0 gradients, 4.1 GFLOPs\n",
      "Model summary: 157 layers, 1765930 parameters, 0 gradients, 4.1 GFLOPs\n",
      "\n",
      "Dataset not found ⚠️, missing paths ['/root/driverless-ml-dev/ml_data/perceptions/fsoco_yolo/images/val']\n",
      "WARNING ⚠️ Benchmark failure for PyTorch: Dataset not found ❌\n",
      "\n",
      "\n",
      "\n",
      "Benchmarks complete (1.10s)\n",
      "    Format Size (MB) mAP50-95 Inference time (ms)\n",
      "0  PyTorch      None     None                None\n",
      "\n",
      "Dataset not found ⚠️, missing paths ['/root/driverless-ml-dev/ml_data/perceptions/fsoco_yolo/images/val']\n",
      "WARNING ⚠️ Benchmark failure for PyTorch: Dataset not found ❌\n",
      "\n",
      "\n",
      "\n",
      "Benchmarks complete (1.10s)\n",
      "    Format Size (MB) mAP50-95 Inference time (ms)\n",
      "0  PyTorch      None     None                None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Format</th>\n",
       "      <th>Size (MB)</th>\n",
       "      <th>mAP50-95</th>\n",
       "      <th>Inference time (ms)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PyTorch</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Format Size (MB) mAP50-95 Inference time (ms)\n",
       "0  PyTorch      None     None                None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /root/driverless-ml-dev/yolov5/runs/bench/y5_single_model_benchmark.csv\n"
     ]
    }
   ],
   "source": [
    "# Running YOLOv5 repo's benchmarks.py\n",
    "from pathlib import Path\n",
    "import sys, pandas as pd, torch\n",
    "\n",
    "\n",
    "YOLO_DIR   = Path(globals().get(\"YOLO_DIR\", \"/root/driverless-ml-dev/yolov5\"))\n",
    "RUNS_TRAIN = YOLO_DIR / \"runs\" / \"train\"\n",
    "RUNS_BENCH = YOLO_DIR / \"runs\" / \"bench\"; RUNS_BENCH.mkdir(parents=True, exist_ok=True)\n",
    "DATA_YAML  = Path(globals().get(\"config_path\", \"/root/driverless-ml-dev/ml_data/perceptions/fsoco_yolo/fsoco.yaml\"))\n",
    "IMG        = int(globals().get(\"IMG_SIZE\", 640))\n",
    "DEVICE     = \"0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "candidates = sorted(RUNS_TRAIN.glob(\"fsoco_*/weights/best.pt\"), key=lambda p: p.stat().st_mtime)\n",
    "assert candidates, \"No trained weights under yolov5/runs/train/fsoco_*/weights/best.pt\"\n",
    "W = candidates[-1]\n",
    "print(f\"Using weights: {W}\\nData: {DATA_YAML}\\nimg: {IMG}  device: {DEVICE}\")\n",
    "\n",
    "sys.path.insert(0, str(YOLO_DIR))\n",
    "try:\n",
    "    import benchmarks as y5_bench        # repo root\n",
    "except Exception:\n",
    "    import utils.benchmarks as y5_bench  # older layout\n",
    "\n",
    "if hasattr(y5_bench, \"parse_opt\"):\n",
    "    y5_bench.parse_opt = lambda: None\n",
    "if hasattr(y5_bench, \"notebook_init\"):\n",
    "    y5_bench.notebook_init = lambda *a, **k: None\n",
    "\n",
    "# Run benchmark (Format, Size (MB), mAP50-95, Inference time (ms))\n",
    "df = y5_bench.run(\n",
    "    weights=W,\n",
    "    imgsz=IMG,\n",
    "    batch_size=1,\n",
    "    data=DATA_YAML,\n",
    "    device=DEVICE,\n",
    "    half=False,\n",
    "    test=False,\n",
    "    pt_only=True,\n",
    "    hard_fail=False\n",
    ")\n",
    "\n",
    "display(df)\n",
    "out_csv = RUNS_BENCH / \"y5_single_model_benchmark.csv\"\n",
    "df.assign(weights=str(W), img=IMG, device=DEVICE).to_csv(out_csv, index=False)\n",
    "print(\"Saved:\", out_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mexport: \u001b[0mdata=../../../yolov5/data/coco128.yaml, weights=['/root/driverless-ml-dev/yolov5/runs/train/fsoco_yolov5s.pt2/weights/best.pt'], imgsz=[640, 640], batch_size=1, device=cpu, half=False, inplace=False, keras=False, optimize=False, int8=False, per_tensor=False, dynamic=False, cache=, simplify=False, mlmodel=False, opset=17, verbose=False, workspace=4, nms=False, agnostic_nms=False, topk_per_class=100, topk_all=100, iou_thres=0.45, conf_thres=0.25, include=['onnx']\n",
      "YOLOv5 🚀 2025-10-10 Python-3.12.3 torch-2.8.0a0+34c6371d24.nv25.08 CPU\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/driverless-ml-dev/yolov5/export.py\", line 1547, in <module>\n",
      "    main(opt)\n",
      "  File \"/root/driverless-ml-dev/yolov5/export.py\", line 1542, in main\n",
      "    run(**vars(opt))\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/driverless-ml-dev/yolov5/export.py\", line 1386, in run\n",
      "    model = attempt_load(weights, device=device, inplace=True, fuse=True)  # load FP32 model\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/driverless-ml-dev/yolov5/models/experimental.py\", line 99, in attempt_load\n",
      "    ckpt = torch_load(attempt_download(w), map_location=\"cpu\")  # load\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/utils/patches.py\", line 120, in torch_load\n",
      "    return torch.load(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1484, in load\n",
      "    with _open_file_like(f, \"rb\") as opened_file:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 759, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 740, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "                     ^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/root/driverless-ml-dev/yolov5/runs/train/fsoco_yolov5s.pt2/weights/best.pt'\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/driverless-ml-dev/yolov5/export.py\", line 1547, in <module>\n",
      "    main(opt)\n",
      "  File \"/root/driverless-ml-dev/yolov5/export.py\", line 1542, in main\n",
      "    run(**vars(opt))\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/driverless-ml-dev/yolov5/export.py\", line 1386, in run\n",
      "    model = attempt_load(weights, device=device, inplace=True, fuse=True)  # load FP32 model\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/root/driverless-ml-dev/yolov5/models/experimental.py\", line 99, in attempt_load\n",
      "    ckpt = torch_load(attempt_download(w), map_location=\"cpu\")  # load\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/utils/patches.py\", line 120, in torch_load\n",
      "    return torch.load(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 1484, in load\n",
      "    with _open_file_like(f, \"rb\") as opened_file:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 759, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/serialization.py\", line 740, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "                     ^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/root/driverless-ml-dev/yolov5/runs/train/fsoco_yolov5s.pt2/weights/best.pt'\n"
     ]
    }
   ],
   "source": [
    "weight = runs[-1] / 'weights' / 'best.pt'\n",
    "!python {YOLO_DIR}/export.py --weights {weight} --include onnx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
