{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv8 FSOCO Training with Ultralytics CLI\n",
    "\n",
    "**Essential steps only:**\n",
    "1. Setup Ultralytics (YOLOv8)\n",
    "2. Convert FSOCO labels (Supervisely YOLO)\n",
    "3. Train\n",
    "4. Inference\n",
    "\n",
    "**Run from:** `driverless-ml-dev/perceptions/camera-pipeline/notebooks/`\n",
    "\n",
    "**Activate venv in root/driverless-ml-dev directory, and connect kernel for this notebook to it**\n",
    "open vsc terminal via ctrl+shift+`\n",
    "\n",
    "first time only:\n",
    "```\n",
    "cd ~/driverless-ml-dev\n",
    "python -m venv venv\n",
    "source venv/bin/activate\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --name driverless-ml --display-name \"Python (driverless-ml)\" --user\n",
    "```\n",
    "then type `>developer reload` in the search bar at the top\n",
    "\n",
    "to activate venv\n",
    "```\n",
    "cd /root/driverless-ml-dev\n",
    "source venv/bin/activate\n",
    "```\n",
    "then type `>developer reload` in the search bar at the top\n",
    "\n",
    "install libraries with: `pip install ultralytics pillow tqdm pyyaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Ultralytics Installation\n",
    "\n",
    "Verify that Ultralytics is installed and CUDA is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to create and install a virtual environment\n",
    "# %cd ~/driverless-ml-dev\n",
    "# !python -m venv venv\n",
    "# !source venv/bin/activate\n",
    "# !venv/bin/python -m pip install --upgrade pip ipykernel\n",
    "# !venv/bin/python -m ipykernel install --name driverless-ml --display-name \"Python (driverless-ml)\" --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activate venv in root/driverless-ml-dev directory, and connect kernel for this notebook to it**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ultralytics pillow tqdm pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "\n",
    "# Paths\n",
    "ROOT = Path.cwd().parent.parent.parent  # driverless-ml-dev/\n",
    "print(ROOT)\n",
    "DATA_DIR = ROOT / 'ml_data'\n",
    "FSOCO_RAW = ROOT / 'ml_data/fsoco_raw'  # download fsoco dataset\n",
    "FSOCO_MOD = ROOT / 'ml_data/fsoco_mod'  # copy of raw for preprocessing\n",
    "FSOCO_YOLO = ROOT / 'ml_data/fsoco_yolo'  # final dataset\n",
    "\n",
    "print(f\"Root: {ROOT}\")\n",
    "print(f\"Data: {DATA_DIR}\")\n",
    "print(f\"Fsoco_raw: {FSOCO_RAW}\")\n",
    "print(f\"Fsoco_mod: {FSOCO_MOD}\")\n",
    "print(f\"Fsoco_yolo: {FSOCO_YOLO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download FSOCO Dataset\n",
    "\n",
    "**Manual step required:**\n",
    "1. Visit: https://fsoco.github.io/fsoco-dataset/download\n",
    "2. Download Bounding Boxes dataset (24GB)\n",
    "3. Extract to: `root/ml-data/perceptions/`\n",
    "\n",
    "Expected structure:\n",
    "```\n",
    "root/ml-data/perceptions/\n",
    "└── fsoco_raw/\n",
    "    ├── team1/ann/       # JSON annotations\n",
    "    ├── team1/img/       # Images\n",
    "    └── meta.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def copy_and_flatten_dataset(source_path: Path, dest_path: Path):\n",
    "    \"\"\"\n",
    "    flattening raw dataset:\n",
    "    source/\n",
    "      - meta.json\n",
    "      - teamA/{ann,img}\n",
    "      - teamB/{ann,img}\n",
    "      ...\n",
    "    ->\n",
    "    dest/\n",
    "      - ann/\n",
    "      - img/\n",
    "      - meta.json\n",
    "    \"\"\"\n",
    "    if not source_path.exists() or not any(source_path.iterdir()):\n",
    "        raise FileNotFoundError(f\"Source dataset not found or empty: {source_path}\")\n",
    "\n",
    "    if dest_path.exists() and any(dest_path.iterdir()):\n",
    "        print(f\"Skipping copy: destination already populated {dest_path}\")\n",
    "        return\n",
    "\n",
    "    dest_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for item in dest_path.iterdir():\n",
    "        if item.is_dir():\n",
    "            shutil.rmtree(item)\n",
    "        else:\n",
    "            item.unlink()\n",
    "\n",
    "    ann_out = dest_path / \"ann\"\n",
    "    img_out = dest_path / \"img\"\n",
    "    ann_out.mkdir(exist_ok=True)\n",
    "    img_out.mkdir(exist_ok=True)\n",
    "\n",
    "    meta_src = source_path / \"meta.json\"\n",
    "    if meta_src.exists():\n",
    "        shutil.copy2(meta_src, dest_path / \"meta.json\")\n",
    "\n",
    "    ann_count = 0\n",
    "    img_count = 0\n",
    "\n",
    "    for team_dir in source_path.iterdir():\n",
    "        print(f\"Processing team directory: {team_dir.name}\")\n",
    "        if not team_dir.is_dir():\n",
    "            print(f\"Skipping non-directory item: {team_dir.name}\")\n",
    "            continue\n",
    "\n",
    "        if team_dir.name in {\"ann\", \"img\"}:\n",
    "            continue\n",
    "\n",
    "        ann_dir = team_dir / \"ann\"\n",
    "        img_dir = team_dir / \"img\"\n",
    "\n",
    "        if ann_dir.exists():\n",
    "            for f in ann_dir.iterdir():\n",
    "                if f.is_file():\n",
    "                    dest = ann_out / f\"{team_dir.name}_{f.name}\"\n",
    "                    shutil.copy2(f, dest)\n",
    "                    ann_count += 1\n",
    "\n",
    "        if img_dir.exists():\n",
    "            for f in img_dir.iterdir():\n",
    "                if f.is_file():\n",
    "                    dest = img_out / f\"{team_dir.name}_{f.name}\"\n",
    "                    shutil.copy2(f, dest)\n",
    "                    img_count += 1\n",
    "\n",
    "    print(f\"Copy+flatten complete {ann_out} ({ann_count} files), {img_out} ({img_count} files)\")\n",
    "\n",
    "\n",
    "raw_exists = FSOCO_RAW.exists()\n",
    "raw_has_content = os.path.isdir(FSOCO_RAW) if raw_exists else False\n",
    "\n",
    "# used later on to skip ratio filtering since it assumes you already ran it\n",
    "mod_exists = FSOCO_MOD.exists()\n",
    "if raw_exists and raw_has_content:\n",
    "    print(\"FSOCO dataset found\")\n",
    "    if FSOCO_MOD.exists() and any(FSOCO_MOD.iterdir()):\n",
    "        print(f\"Skipping flatten: working copy already populated {FSOCO_MOD}\")\n",
    "    else:\n",
    "        for item in FSOCO_RAW.iterdir():\n",
    "            print(f\"  - {item.name}\")\n",
    "        FSOCO_MOD.mkdir(parents=True, exist_ok=True)\n",
    "        copy_and_flatten_dataset(FSOCO_RAW, FSOCO_MOD)\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Download FSOCO dataset to: {FSOCO_RAW}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not FSOCO_MOD.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Working dataset directory not found: {FSOCO_MOD}. Run the preparation step and ensure the directory exists.\"\n",
    "    )\n",
    "\n",
    "if not any(FSOCO_MOD.iterdir()):\n",
    "    raise RuntimeError(\n",
    "        f\"Working dataset directory is empty: {FSOCO_MOD}. Run the preparation step to copy data before proceeding.\"\n",
    "    )\n",
    "\n",
    "working_dataset = FSOCO_MOD\n",
    "print(f\"Inspecting dataset at: {working_dataset}\")\n",
    "\n",
    "bb_path = working_dataset / 'ann'\n",
    "print(bb_path)\n",
    "print(\"ann contents:\")\n",
    "if bb_path.exists():\n",
    "    items = list(os.listdir(bb_path))[:5]\n",
    "    for item in items:\n",
    "        print(f\"  {item}\")\n",
    "else:\n",
    "    print(\"no ann directory found\")\n",
    "\n",
    "img_path = working_dataset / 'img'\n",
    "\n",
    "print(\"\\nimg contents:\")\n",
    "if img_path.exists():\n",
    "    items = list(os.listdir(img_path))[:5]\n",
    "    for item in items:\n",
    "        print(f\"  {item}\")\n",
    "else:\n",
    "    print(\"no image directory found\")\n",
    "\n",
    "print(\"\\nmeta.json:\")\n",
    "meta_path = working_dataset / 'meta.json'\n",
    "\n",
    "if meta_path.exists():\n",
    "    import json\n",
    "    meta = json.load(open(meta_path))\n",
    "    print(json.dumps(meta, indent=2)[:500])\n",
    "else:\n",
    "    print(\"meta.json not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filter Images by Aspect Ratio\n",
    "\n",
    "Remove images that don't meet minimum aspect ratio requirements (width/height)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "def filter_images_by_aspect_ratio(fsoco_mod, min_ratio=1.0, max_ratio=3.0):\n",
    "    img_dir = fsoco_mod / 'images'\n",
    "    if not img_dir.exists():\n",
    "        img_dir = fsoco_mod / 'img'\n",
    "    ann_dir = fsoco_mod / 'bounding_boxes'\n",
    "    if not ann_dir.exists():\n",
    "        ann_dir = fsoco_mod / 'ann'\n",
    "    \n",
    "    if not img_dir.exists():\n",
    "        print(f\"Image directory not found: {img_dir}\")\n",
    "        return [], 0\n",
    "    \n",
    "    images = list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png'))\n",
    "    print(f\"Total images found: {len(images)}\")\n",
    "    \n",
    "    filtered_images = []\n",
    "    removed_images = []\n",
    "    kept_dimensions = []\n",
    "    removed_dimensions = []\n",
    "    kept_aspect_ratios = []\n",
    "    removed_aspect_ratios = []\n",
    "    \n",
    "    for img_path in tqdm(images, desc=\"Filtering images by aspect ratio\"):\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            w, h = img.size\n",
    "            aspect_ratio = w / h\n",
    "            \n",
    "            if min_ratio <= aspect_ratio <= max_ratio:\n",
    "                filtered_images.append(img_path)\n",
    "                kept_dimensions.append((w, h))\n",
    "                kept_aspect_ratios.append(aspect_ratio)\n",
    "            else:\n",
    "                removed_images.append((img_path, aspect_ratio, w, h))\n",
    "                removed_dimensions.append((w, h))\n",
    "                removed_aspect_ratios.append(aspect_ratio)\n",
    "                \n",
    "                img_path.unlink()\n",
    "                \n",
    "                ann_path = ann_dir / f\"{img_path.name}.json\"\n",
    "                if not ann_path.exists():\n",
    "                    ann_path = ann_dir / f\"{img_path.stem}.json\"\n",
    "                if ann_path.exists():\n",
    "                    ann_path.unlink()\n",
    "                \n",
    "                print(f\"Removed: {img_path.name} (aspect ratio: {aspect_ratio:.2f}, dims: {w}x{h})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path.name}: {e}\")\n",
    "            continue\n",
    "    #- Print detailed statistics -\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FILTERING STATISTICS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"\\nOVERALL SUMMARY:\")\n",
    "    print(f\"  Total images processed:  {len(images)}\")\n",
    "    if images:\n",
    "        print(f\"  Images kept:           {len(filtered_images)} ({100*len(filtered_images)/len(images):.1f}%)\")\n",
    "        print(f\"  Images filtered out:   {len(removed_images)} ({100*len(removed_images)/len(images):.1f}%)\")\n",
    "    else:\n",
    "        print(\"  Images kept:           0\")\n",
    "        print(\"  Images filtered out:   0\")\n",
    "    print(f\"  Aspect ratio range:      {min_ratio} - {max_ratio}\")\n",
    "    \n",
    "    if kept_aspect_ratios:\n",
    "        print(f\"\\nKEPT IMAGES - ASPECT RATIO STATS:\")\n",
    "        print(f\"  Min aspect ratio:  {min(kept_aspect_ratios):.2f}\")\n",
    "        print(f\"  Max aspect ratio:  {max(kept_aspect_ratios):.2f}\")\n",
    "        print(f\"  Avg aspect ratio:  {sum(kept_aspect_ratios)/len(kept_aspect_ratios):.2f}\")\n",
    "    \n",
    "    if kept_dimensions:\n",
    "        print(f\"\\nKEPT IMAGES - DIMENSION STATS:\")\n",
    "        dim_counter = Counter(kept_dimensions)\n",
    "        top_dims = dim_counter.most_common(10)\n",
    "        print(f\"  Unique dimensions: {len(dim_counter)}\")\n",
    "        print(f\"  Top 10 dimensions:\")\n",
    "        for (w, h), count in top_dims:\n",
    "            ratio = w/h\n",
    "            print(f\"    {w}x{h} (ratio {ratio:.2f}): {count} images\")\n",
    "    \n",
    "    if removed_aspect_ratios:\n",
    "        print(f\"\\nREMOVED IMAGES - ASPECT RATIO STATS:\")\n",
    "        print(f\"  Min aspect ratio:  {min(removed_aspect_ratios):.2f}\")\n",
    "        print(f\"  Max aspect ratio:  {max(removed_aspect_ratios):.2f}\")\n",
    "        print(f\"  Avg aspect ratio:  {sum(removed_aspect_ratios)/len(removed_aspect_ratios):.2f}\")\n",
    "    \n",
    "    if removed_dimensions:\n",
    "        print(f\"\\nREMOVED IMAGES - DIMENSION STATS:\")\n",
    "        dim_counter = Counter(removed_dimensions)\n",
    "        print(f\"  Unique dimensions: {len(dim_counter)}\")\n",
    "        print(f\"  All removed dimensions:\")\n",
    "        for (w, h), count in sorted(dim_counter.items(), key=lambda x: x[1], reverse=True):\n",
    "            ratio = w/h\n",
    "            print(f\"    {w}x{h} (ratio {ratio:.2f}): {count} images\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "    \n",
    "    return filtered_images, len(removed_images)\n",
    "\n",
    "MIN_ASPECT_RATIO = 1.25  # minimum width/height ratio\n",
    "MAX_ASPECT_RATIO = 1.80  # maximum width/height ratio\n",
    "\n",
    "if not FSOCO_MOD.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Working dataset directory not found: {FSOCO_MOD}. Create it and rerun the preparation step before filtering.\"\n",
    "    )\n",
    "\n",
    "if not any(FSOCO_MOD.iterdir()):\n",
    "    raise RuntimeError(\n",
    "        f\"Working dataset directory is empty: {FSOCO_MOD}. Run the preparation step to copy data before filtering.\"\n",
    "    )\n",
    "\n",
    "if not mod_exists:\n",
    "    print(f\"Using working dataset: {FSOCO_MOD}\")\n",
    "    filtered_imgs, removed = filter_images_by_aspect_ratio(\n",
    "        FSOCO_MOD,\n",
    "        min_ratio=MIN_ASPECT_RATIO,\n",
    "        max_ratio=MAX_ASPECT_RATIO\n",
    "    )\n",
    "    mod_exists = True\n",
    "    print(\"Aspect ratio filtering complete. Flag updated to skip on next run.\")\n",
    "else:\n",
    "    print(\"Skipping aspect ratio filtering: already applied previously.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convert Labels: Supervisely YOLO\n",
    "\n",
    "YOLO format: `<class_id> <x_center> <y_center> <width> <height>` (all normalized 0-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "def convert_supervisely_to_yolo(fsoco_raw, fsoco_yolo, split=(0.8, 0.1, 0.1)):\n",
    "    for s in ['train', 'val', 'test']:\n",
    "        (fsoco_yolo / 'images' / s).mkdir(parents=True, exist_ok=True)\n",
    "        (fsoco_yolo / 'labels' / s).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # hard code class names\n",
    "    classes = ['blue_cone', 'unknown_cone', 'orange_cone', 'large_orange_cone', 'yellow_cone']\n",
    "    class_map = {name: i for i, name in enumerate(classes)}\n",
    "    print(f\"Classes (bounding boxes only): {classes}\\n\")\n",
    "    print(f\"Class mapping: {class_map}\\n\")\n",
    "\n",
    "    ann_dir = fsoco_raw / 'ann'\n",
    "    img_dir = fsoco_raw / 'img'\n",
    "    \n",
    "    ann_files = list(ann_dir.glob('*.json'))\n",
    "    print(f\"Total annotations: {len(ann_files)}\")\n",
    "    \n",
    "    random.shuffle(ann_files)\n",
    "    n1 = int(len(ann_files) * split[0])\n",
    "    n2 = int(len(ann_files) * (split[0] + split[1]))\n",
    "    splits = {\n",
    "        'train': ann_files[:n1],\n",
    "        'val': ann_files[n1:n2],\n",
    "        'test': ann_files[n2:]\n",
    "    }\n",
    "    \n",
    "    for split_name, anns in splits.items():\n",
    "        print(f\"\\nConverting {split_name}: {len(anns)} images\")\n",
    "        \n",
    "        for ann_path in tqdm(anns):\n",
    "            img_name = ann_path.stem\n",
    "            img_path = img_dir / img_name\n",
    "            \n",
    "            if not img_path.exists():\n",
    "                alt_name = img_name.replace('.jpg', '.png') if '.jpg' in img_name else img_name.replace('.png', '.jpg')\n",
    "                img_path = img_dir / alt_name\n",
    "                if not img_path.exists():\n",
    "                    continue\n",
    "\n",
    "            img = Image.open(img_path)\n",
    "            w, h = img.size\n",
    "\n",
    "            ann = json.load(open(ann_path))\n",
    "\n",
    "            yolo_labels = []\n",
    "            for obj in ann.get('objects', []):\n",
    "                cls = obj['classTitle']\n",
    "                if cls not in class_map or obj['geometryType'] != 'rectangle':\n",
    "                    continue\n",
    "                \n",
    "                points = obj['points']['exterior']\n",
    "                x1, y1 = points[0]\n",
    "                x2, y2 = points[1]\n",
    "\n",
    "                x_center = ((x1 + x2) / 2) / w\n",
    "                y_center = ((y1 + y2) / 2) / h\n",
    "                width = abs(x2 - x1) / w\n",
    "                height = abs(y2 - y1) / h\n",
    "\n",
    "                x_center = max(0, min(1, x_center))\n",
    "                y_center = max(0, min(1, y_center))\n",
    "                width = max(0, min(1, width))\n",
    "                height = max(0, min(1, height))\n",
    "                \n",
    "                yolo_labels.append(f\"{class_map[cls]} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\")\n",
    "            \n",
    "            if yolo_labels:\n",
    "                shutil.copy(img_path, fsoco_yolo / 'images' / split_name / img_path.name)\n",
    "                with open(fsoco_yolo / 'labels' / split_name / f\"{img_path.stem}.txt\", 'w') as f:\n",
    "                    f.write('\\n'.join(yolo_labels))\n",
    "    \n",
    "    print(\"\\nConversion complete\")\n",
    "    print(f\"\\nDataset splits:\")\n",
    "    for split_name in ['train', 'val', 'test']:\n",
    "        n_imgs = len(list((fsoco_yolo / 'images' / split_name).glob('*.[jp][pn]g')))\n",
    "        print(f\"  {split_name}: {n_imgs} images\")\n",
    "    \n",
    "    return classes\n",
    "\n",
    "classes = None\n",
    "if (FSOCO_MOD / 'meta.json').exists():\n",
    "    try:\n",
    "        meta = json.load(open(FSOCO_MOD / 'meta.json'))\n",
    "        classes = [c['title'] for c in meta['classes'] if c['shape'] == 'rectangle']\n",
    "        print(f\"Loaded {len(classes)} classes from meta.json: {classes}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Failed to load classes from meta.json: {e}\")\n",
    "\n",
    "yolo_exists = FSOCO_YOLO.exists() and any(FSOCO_YOLO.iterdir())\n",
    "\n",
    "if not (FSOCO_MOD.exists() and any(FSOCO_MOD.iterdir())):\n",
    "    raise FileNotFoundError(\n",
    "        f\"Working dataset not found or empty: {FSOCO_MOD}\\n\"\n",
    "        f\"Please run the dataset preparation step first (cell 10).\"\n",
    "    )\n",
    "\n",
    "if yolo_exists:\n",
    "    print(f\"Skipping conversion: YOLO dataset already populated {FSOCO_YOLO}\")\n",
    "else:\n",
    "    print(f\"Using working dataset: {FSOCO_MOD}\")\n",
    "    classes = convert_supervisely_to_yolo(FSOCO_MOD, FSOCO_YOLO)\n",
    "\n",
    "if classes is None:\n",
    "    raise RuntimeError(\n",
    "        \"Failed to load class definitions.\\n\"\n",
    "        f\"Ensure meta.json exists in {FSOCO_MOD} or run the conversion.\"\n",
    "    )\n",
    "\n",
    "print(f\"\\nFinal classes list ({len(classes)} classes): {classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Dataset Config (YAML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "if classes is None:\n",
    "    raise RuntimeError(\n",
    "        \"Classes not defined. Please run the label conversion step first.\"\n",
    "    )\n",
    "\n",
    "config_path = FSOCO_YOLO / 'fsoco.yaml'\n",
    "\n",
    "if yolo_exists and config_path.exists():\n",
    "    print(f\"Config already exists: {config_path}\")\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    print(yaml.dump(config, default_flow_style=False))\n",
    "else:\n",
    "    config = {\n",
    "        'path': str(FSOCO_YOLO.absolute()),\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/val',\n",
    "        'test': 'images/test',\n",
    "        'nc': len(classes),\n",
    "        'names': classes\n",
    "    }\n",
    "    \n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "    \n",
    "    print(\"Config saved:\", config_path)\n",
    "    print(yaml.dump(config, default_flow_style=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. YOLOv8 Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "MODEL = 'yolov8s.pt'  # Options: yolov8n, yolov8s, yolov8m, yolov8l, yolov8x; change number to 10, 11\n",
    "EPOCHS = 100\n",
    "BATCH = -1  # auto batch size\n",
    "IMG_SIZE = 640\n",
    "DEVICE = \"0\" if torch.cuda.is_available() else \"cpu\"\n",
    "PROJECT = 'runs/detect'\n",
    "NAME = 'fsoco_train'\n",
    "#add early stopping\n",
    "\n",
    "print(f\"Model: {MODEL}, Epochs: {EPOCHS}, Batch: {BATCH}, Image size: {IMG_SIZE}, Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/yolo\", line 3, in <module>\n",
      "    from ultralytics.cfg import entrypoint\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/__init__.py\", line 12, in <module>\n",
      "    from ultralytics.utils import ASSETS, SETTINGS\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ultralytics/utils/__init__.py\", line 25, in <module>\n",
      "    import torch\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/__init__.py\", line 2073, in <module>\n",
      "    _C._initExtension(_manager_path())\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py\", line 1458, in <module>\n",
      "    from .memory import *  # noqa: F403\n",
      "    ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/cuda/memory.py\", line 599, in <module>\n",
      "    @deprecated(\n",
      "     ^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/typing_extensions.py\", line 2949, in __call__\n",
      "    import asyncio.coroutines\n",
      "  File \"/usr/lib/python3.12/asyncio/__init__.py\", line 8, in <module>\n",
      "    from .base_events import *\n",
      "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 35, in <module>\n",
      "    import ssl\n",
      "  File \"/usr/lib/python3.12/ssl.py\", line 160, in <module>\n",
      "    class TLSVersion:\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!yolo settings tensorboard=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Train YOLOv8 Model\n",
    "\n",
    "Using the Ultralytics CLI to train the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo detect train data={config_path} model={MODEL} epochs={EPOCHS} imgsz={IMG_SIZE} batch={BATCH} device={DEVICE} project={PROJECT} name={NAME}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IPImage\n",
    "from pathlib import Path\n",
    "\n",
    "runs_dir = Path('runs/detect')\n",
    "if runs_dir.exists():\n",
    "    runs = sorted(runs_dir.glob('fsoco_train*'), key=lambda x: x.stat().st_mtime)\n",
    "    if runs:\n",
    "        latest = runs[-1]\n",
    "        print(f\"Training run: {latest.name}\\n\")\n",
    "\n",
    "        results_img = latest / 'results.png'\n",
    "        if results_img.exists():\n",
    "            display(IPImage(filename=str(results_img)))\n",
    "        \n",
    "        print(f\"\\nWeights: {latest / 'weights' / 'best.pt'}\")\n",
    "\n",
    "        with mlflow.start_run(run_name=latest.name):\n",
    "            mlflow.log_params({\n",
    "                \"model\": MODEL,\n",
    "                \"epochs\": EPOCHS,\n",
    "                \"batch_size\": BATCH,\n",
    "                \"img_size\": IMG_SIZE,\n",
    "                \"device\": DEVICE\n",
    "            })\n",
    "            \n",
    "            results_csv = latest / 'results.csv'\n",
    "            if results_csv.exists():\n",
    "                import pandas as pd\n",
    "                df = pd.read_csv(results_csv)\n",
    "                if not df.empty:\n",
    "                    last_row = df.iloc[-1]\n",
    "                    for col in df.columns:\n",
    "                        if col.strip() and col.strip() != 'epoch':\n",
    "                            try:\n",
    "                                mlflow.log_metric(col.strip(), float(last_row[col]))\n",
    "                            except:\n",
    "                                pass\n",
    "        \n",
    "        mlflow.end_run()\n",
    "    else:\n",
    "        print(\"No training runs found\")\n",
    "else:\n",
    "    print(\"No runs directory found. Train the model first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to tensorrt\n",
    "from pathlib import Path\n",
    "\n",
    "runs_dir = Path('runs/detect')\n",
    "if runs_dir.exists():\n",
    "    runs = sorted(runs_dir.glob('fsoco_train*'), key=lambda x: x.stat().st_mtime)\n",
    "    if runs:\n",
    "        weight = runs[-1] / 'weights' / 'best.pt'\n",
    "        print(f\"Exporting weights: {weight}\")\n",
    "        !yolo export model={weight} format=engine device={DEVICE}\n",
    "    else:\n",
    "        print(\"No training runs found. Train the model first.\")\n",
    "else:\n",
    "    print(\"No runs directory found. Train the model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Export\n",
    "\n",
    "Export the trained model to formats:\n",
    "- `onnx`: ONNX format \n",
    "- `engine`: TensorRT (for jetson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "from pathlib import Path\n",
    "\n",
    "runs_dir = Path('runs/detect')\n",
    "if runs_dir.exists():\n",
    "    runs = sorted(runs_dir.glob('fsoco_train*'), key=lambda x: x.stat().st_mtime)\n",
    "    if runs:\n",
    "        weight = runs[-1] / 'weights' / 'best.pt'\n",
    "        print(f\"Exporting weights: {weight}\\n\")\n",
    "\n",
    "        formats = [\n",
    "            'onnx',      # ONNX format\n",
    "            # 'engine',  # TensorRT format\n",
    "        ]\n",
    "        \n",
    "        for fmt in formats:\n",
    "            print(f\"\\nExporting to {fmt}...\")\n",
    "            !yolo export model={weight} format={fmt} imgsz={IMG_SIZE}\n",
    "        \n",
    "        print(\"\\nExport complete!\")\n",
    "    else:\n",
    "        print(\"No training runs found. Train the model first.\")\n",
    "else:\n",
    "    print(\"No runs directory found. Train the model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "runs_dir = Path('runs/detect')\n",
    "if runs_dir.exists():\n",
    "    runs = sorted(runs_dir.glob('fsoco_train*'), key=lambda x: x.stat().st_mtime)\n",
    "    if runs:\n",
    "        weights = runs[-1] / 'weights' / 'best.pt'\n",
    "        test_imgs = FSOCO_YOLO / 'images' / 'test'\n",
    "        \n",
    "        if weights.exists() and test_imgs.exists():\n",
    "            print(\"Running inference with YOLOv8...\\n\")\n",
    "            #inference\n",
    "            !yolo detect predict model={weights} source={test_imgs} imgsz={IMG_SIZE} conf=0.25 save=True name=fsoco_inference project=runs/detect\n",
    "\n",
    "            # results\n",
    "            detect_runs = sorted(Path('runs/detect').glob('fsoco_inference*'), \n",
    "                               key=lambda x: x.stat().st_mtime)\n",
    "            if detect_runs:\n",
    "                print(f\"\\nResults saved to: {detect_runs[-1]}\")\n",
    "\n",
    "                results = list(detect_runs[-1].glob('*.jpg'))[:3]  # Show first 3 results\n",
    "                for r in results:\n",
    "                    print(f\"\\nShowing: {r.name}\")\n",
    "                    display(IPImage(filename=str(r), width=800))\n",
    "        else:\n",
    "            print(\"Missing weights or test images\")\n",
    "    else:\n",
    "        print(\"No training runs found. Train model first.\")\n",
    "else:\n",
    "    print(\"No runs directory found. Train model first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Validate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "runs_dir = Path('runs/detect')\n",
    "if runs_dir.exists():\n",
    "    runs = sorted(runs_dir.glob('fsoco_train*'), key=lambda x: x.stat().st_mtime)\n",
    "    if runs:\n",
    "        weights = runs[-1] / 'weights' / 'best.pt'\n",
    "        \n",
    "        if weights.exists():\n",
    "            print(\"Running validation...\\n\")\n",
    "            \n",
    "            # validation\n",
    "            !yolo detect val model={weights} data={config_path} imgsz={IMG_SIZE} batch={BATCH} device={DEVICE} save_json=True split=val\n",
    "            \n",
    "            print(\"\\nValidation complete!\")\n",
    "        else:\n",
    "            print(\"Weights not found\")\n",
    "    else:\n",
    "        print(\"No training runs found. Train model first.\")\n",
    "else:\n",
    "    print(\"No runs directory found. Train model first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# benchmarking\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "runs_dir = Path('runs/detect')\n",
    "if runs_dir.exists():\n",
    "    candidates = sorted(runs_dir.glob(\"fsoco_train*/weights/best.pt\"), key=lambda p: p.stat().st_mtime)\n",
    "    \n",
    "    if candidates:\n",
    "        W = candidates[-1]\n",
    "        DATA_YAML = config_path\n",
    "        IMG = IMG_SIZE\n",
    "        DEVICE = \"0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        print(f\"Benchmarking weights: {W}\")\n",
    "        print(f\"Data: {DATA_YAML}\")\n",
    "        print(f\"Image size: {IMG}\")\n",
    "        print(f\"Device: {DEVICE}\\n\")\n",
    "        \n",
    "        # benchmark\n",
    "        !yolo detect val model={W} data={DATA_YAML} imgsz={IMG} batch=1 device={DEVICE} save_json=True\n",
    "        \n",
    "        # speed test\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"Running speed benchmark...\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        # speed\n",
    "        !yolo benchmark model={W} data={DATA_YAML} imgsz={IMG} device={DEVICE}\n",
    "        \n",
    "    else:\n",
    "        print(\"No trained weights found. Train the model first.\")\n",
    "else:\n",
    "    print(\"No runs directory found. Train the model first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
