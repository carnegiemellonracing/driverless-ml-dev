{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLOv5 FSOCO Training\n",
    "\n",
    "**Essential steps only:**\n",
    "1. Setup YOLOv5\n",
    "2. Convert FSOCO labels (Supervisely → YOLO)\n",
    "3. Train\n",
    "4. Inference\n",
    "\n",
    "**Run from:** `driverless-ml-dev/perceptions/camera-pipeline/notebooks/`\n",
    "\n",
    "**Activate venv in root/driverless-ml-dev directory, and connect kernel for this notebook to it**\n",
    "open vsc terminal via ctrl+shift+`\n",
    "\n",
    "first time only:\n",
    "```\n",
    "cd root/driverless-ml-dev\n",
    "python -m venv venv\n",
    "source venv/bin/activate\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --name driverless-ml --display-name \"Python (driverless-ml)\" --user\n",
    "```\n",
    "then type `>developer reload` in the search bar at the top\n",
    "\n",
    "to activate venv\n",
    "```\n",
    "cd /root/driverless-ml-dev\n",
    "source venv/bin/activate\n",
    "```\n",
    "then type `>developer reload` in the search bar at the top\n",
    "\n",
    "install libraries with: `pip install pillow tqdm pyyaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/driverless-ml-dev\n",
      "Requirement already satisfied: pip in ./venv/lib/python3.12/site-packages (25.2)\n",
      "Requirement already satisfied: ipykernel in ./venv/lib/python3.12/site-packages (6.30.1)\n",
      "Requirement already satisfied: comm>=0.1.1 in ./venv/lib/python3.12/site-packages (from ipykernel) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./venv/lib/python3.12/site-packages (from ipykernel) (1.8.17)\n",
      "Requirement already satisfied: ipython>=7.23.1 in ./venv/lib/python3.12/site-packages (from ipykernel) (9.6.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in ./venv/lib/python3.12/site-packages (from ipykernel) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./venv/lib/python3.12/site-packages (from ipykernel) (5.8.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./venv/lib/python3.12/site-packages (from ipykernel) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in ./venv/lib/python3.12/site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging>=22 in ./venv/lib/python3.12/site-packages (from ipykernel) (25.0)\n",
      "Requirement already satisfied: psutil>=5.7 in ./venv/lib/python3.12/site-packages (from ipykernel) (7.1.0)\n",
      "Requirement already satisfied: pyzmq>=25 in ./venv/lib/python3.12/site-packages (from ipykernel) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in ./venv/lib/python3.12/site-packages (from ipykernel) (6.5.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in ./venv/lib/python3.12/site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: decorator in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (2.19.2)\n",
      "Requirement already satisfied: stack_data in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from jupyter-client>=8.0.0->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./venv/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.5.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (0.2.3)\n",
      "Installed kernelspec driverless-ml in /root/.local/share/jupyter/kernels/driverless-ml\n",
      "/root\n"
     ]
    }
   ],
   "source": [
    "%cd ~/driverless-ml-dev\n",
    "!python -m venv venv\n",
    "!source venv/bin/activate\n",
    "!venv/bin/python -m pip install --upgrade pip ipykernel\n",
    "!venv/bin/python -m ipykernel install --name driverless-ml --display-name \"Python (driverless-ml)\" --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activate venv in root/driverless-ml-dev directory, and connect kernel for this notebook to it**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "Root: /\n",
      "YOLOv5: /yolov5\n",
      "Data: /ml_data\n",
      "Fsoco_raw: /ml_data/perceptions/fsoco_raw\n",
      "Fsoco_mod: /ml_data/perceptions/fsoco_mod\n",
      "Fsoco_yolo: /ml_data/perceptions/fsoco_yolo\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "ROOT = Path.cwd().parent.parent.parent  # driverless-ml-dev/\n",
    "print(ROOT)\n",
    "YOLO_DIR = ROOT / 'yolov5'\n",
    "DATA_DIR = ROOT / 'ml_data'\n",
    "FSOCO_RAW = ROOT / 'ml_data/perceptions/fsoco_raw'  # Download FSOCO here\n",
    "FSOCO_YOLO = ROOT / 'ml_data/perceptions/fsoco_yolo'  # Converted dataset\n",
    "FSOCO_MOD = ROOT / 'ml_data/perceptions/fsoco_mod'  # Working copy for preprocessing\n",
    "\n",
    "print(f\"Root: {ROOT}\")\n",
    "print(f\"YOLOv5: {YOLO_DIR}\")\n",
    "print(f\"Data: {DATA_DIR}\")\n",
    "print(f\"Fsoco_raw: {FSOCO_RAW}\")\n",
    "print(f\"Fsoco_mod: {FSOCO_MOD}\")\n",
    "print(f\"Fsoco_yolo: {FSOCO_YOLO}\")\n",
    "for item in FSOCO_RAW.iterdir():\n",
    "    print(f\"  - {item.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ YOLOv5 exists\n",
      "Requirement already satisfied: gitpython>=3.1.30 in /usr/local/lib/python3.12/dist-packages (from -r /yolov5/requirements.txt (line 5)) (3.1.45)\n",
      "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.12/dist-packages (from -r /yolov5/requirements.txt (line 6)) (3.10.5)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.12/dist-packages (from -r /yolov5/requirements.txt (line 7)) (1.26.4)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from -r /yolov5/requirements.txt (line 8)) (4.11.0.86)\n",
      "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.12/dist-packages (from -r /yolov5/requirements.txt (line 9)) (11.3.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from -r /yolov5/requirements.txt (line 10)) (7.0.0)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from -r /yolov5/requirements.txt (line 11)) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from -r /yolov5/requirements.txt (line 12)) (2.32.4)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from -r /yolov5/requirements.txt (line 13)) (1.15.3)\n",
      "Requirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from -r /yolov5/requirements.txt (line 14)) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from -r /yolov5/requirements.txt (line 15)) (2.8.0a0+34c6371d24.nv25.8)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from -r /yolov5/requirements.txt (line 16)) (0.23.0a0+428a54c9)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from -r /yolov5/requirements.txt (line 17)) (4.67.1)\n",
      "Requirement already satisfied: ultralytics>=8.2.64 in /usr/local/lib/python3.12/dist-packages (from -r /yolov5/requirements.txt (line 18)) (8.3.205)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.12/dist-packages (from -r /yolov5/requirements.txt (line 27)) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from -r /yolov5/requirements.txt (line 28)) (0.13.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from -r /yolov5/requirements.txt (line 42)) (23.2)\n",
      "Requirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.12/dist-packages (from -r /yolov5/requirements.txt (line 43)) (79.0.1)\n",
      "Requirement already satisfied: urllib3>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from -r /yolov5/requirements.txt (line 51)) (2.5.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython>=3.1.30->-r /yolov5/requirements.txt (line 5)) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r /yolov5/requirements.txt (line 5)) (5.0.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->-r /yolov5/requirements.txt (line 6)) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->-r /yolov5/requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->-r /yolov5/requirements.txt (line 6)) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->-r /yolov5/requirements.txt (line 6)) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->-r /yolov5/requirements.txt (line 6)) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3->-r /yolov5/requirements.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->-r /yolov5/requirements.txt (line 12)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->-r /yolov5/requirements.txt (line 12)) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->-r /yolov5/requirements.txt (line 12)) (2025.8.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->-r /yolov5/requirements.txt (line 15)) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->-r /yolov5/requirements.txt (line 15)) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->-r /yolov5/requirements.txt (line 15)) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->-r /yolov5/requirements.txt (line 15)) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->-r /yolov5/requirements.txt (line 15)) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->-r /yolov5/requirements.txt (line 15)) (2025.7.0)\n",
      "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics>=8.2.64->-r /yolov5/requirements.txt (line 18)) (1.28.1)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics>=8.2.64->-r /yolov5/requirements.txt (line 18)) (2.0.17)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.4->-r /yolov5/requirements.txt (line 27)) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.4->-r /yolov5/requirements.txt (line 27)) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r /yolov5/requirements.txt (line 6)) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->-r /yolov5/requirements.txt (line 15)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->-r /yolov5/requirements.txt (line 15)) (3.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "✓ Dependencies installed\n"
     ]
    }
   ],
   "source": [
    "# Clone YOLOv5\n",
    "if not YOLO_DIR.exists():\n",
    "    !cd {ROOT} && git clone https://github.com/ultralytics/yolov5.git\n",
    "    print(\"✓ YOLOv5 cloned\")\n",
    "else:\n",
    "    print(\"✓ YOLOv5 exists\")\n",
    "\n",
    "# Install requirements\n",
    "%pip install -r {YOLO_DIR}/requirements.txt\n",
    "print(\"✓ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download FSOCO Dataset\n",
    "\n",
    "**Manual step required:**\n",
    "1. Visit: https://www.fsoco-dataset.com/download\n",
    "2. Download dataset (Supervisely format)\n",
    "3. Extract to: `data/fsoco_raw/`\n",
    "\n",
    "Expected structure:\n",
    "```\n",
    "data/fsoco_raw/\n",
    "└── dataset_name/\n",
    "    ├── ann/       # JSON annotations\n",
    "    ├── img/       # Images\n",
    "    └── meta.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "/ml_data/perceptions/fsoco_raw\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def flatten_dataset_structure(dataset_path: str):\n",
    "    \"\"\"\n",
    "    change copies and redoes the downloaded dataset structure from:\n",
    "        dataset/\n",
    "          - meta.json\n",
    "          - team1/ann, team1/img\n",
    "          - team2/ann, team2/img\n",
    "          ...\n",
    "    to:\n",
    "        dataset/\n",
    "          - ann/\n",
    "          - img/\n",
    "          - meta.json\n",
    "    \"\"\"\n",
    "    root = Path(dataset_path)\n",
    "    ann_out = root / \"ann\"\n",
    "    img_out = root / \"img\"\n",
    "\n",
    "    #check output dirs\n",
    "    ann_out.mkdir(exist_ok=True)\n",
    "    img_out.mkdir(exist_ok=True)\n",
    "\n",
    "    for team_dir in root.iterdir():\n",
    "        if team_dir.is_dir() and team_dir.name not in {\"ann\", \"img\"}:\n",
    "            ann_dir = team_dir / \"ann\"\n",
    "            img_dir = team_dir / \"img\"\n",
    "\n",
    "            if ann_dir.exists():\n",
    "                for file in ann_dir.iterdir():\n",
    "                    dest = ann_out / f\"{team_dir.name}_{file.name}\"\n",
    "                    shutil.move(str(file), dest)\n",
    "\n",
    "            # copy img files\n",
    "            if img_dir.exists():\n",
    "                for file in img_dir.iterdir():\n",
    "                    dest = img_out / f\"{team_dir.name}_{file.name}\"\n",
    "                    shutil.move(str(file), dest)\n",
    "\n",
    "            # Clean up empty team dir\n",
    "            shutil.rmtree(team_dir)\n",
    "\n",
    "    print(f\"Flattening complete. Files moved into {ann_out} and {img_out}\")\n",
    "\n",
    "\n",
    "def prepare_working_copy(source_path: Path, dest_path: Path):\n",
    "    \"\"\"Copy the raw dataset into a working directory before mutating it.\"\"\"\n",
    "    if not dest_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"✗ Working dataset directory not found: {dest_path}\\nCreate this directory before running the preprocessing pipeline.\"\n",
    "        )\n",
    "\n",
    "    if not source_path.exists() or not any(source_path.iterdir()):\n",
    "        raise FileNotFoundError(\n",
    "            f\"✗ Source dataset not found or empty: {source_path}\\nDownload and extract the FSOCO dataset first.\"\n",
    "        )\n",
    "\n",
    "    print(f\"Preparing working dataset at: {dest_path}\")\n",
    "\n",
    "    # Clear existing contents in the working directory\n",
    "    for item in dest_path.iterdir():\n",
    "        if item.is_dir():\n",
    "            shutil.rmtree(item)\n",
    "        else:\n",
    "            item.unlink()\n",
    "\n",
    "    # Copy fresh contents from the source dataset\n",
    "    for item in source_path.iterdir():\n",
    "        target = dest_path / item.name\n",
    "        if item.is_dir():\n",
    "            shutil.copytree(item, target)\n",
    "        else:\n",
    "            shutil.copy2(item, target)\n",
    "\n",
    "    print(\"✓ Working copy created\")\n",
    "\n",
    "\n",
    "raw_exists = FSOCO_RAW.exists()\n",
    "print(any(FSOCO_RAW.iterdir()))\n",
    "print(FSOCO_RAW)\n",
    "raw_has_content = any(FSOCO_RAW.iterdir()) if raw_exists else False\n",
    "print(raw_exists)\n",
    "print(raw_has_content)\n",
    "\n",
    "if raw_exists and raw_has_content:\n",
    "    print(\"✓ FSOCO dataset found\")\n",
    "    for item in FSOCO_RAW.iterdir():\n",
    "        print(f\"  - {item.name}\")\n",
    "    print(FSOCO_MOD)\n",
    "    FSOCO_MOD.mkdir(parents=True, exist_ok=True)\n",
    "    prepare_working_copy(FSOCO_RAW, FSOCO_MOD)\n",
    "    flatten_dataset_structure(FSOCO_MOD)\n",
    "# else:\n",
    "#     raise FileNotFoundError(f\"✗ Download FSOCO dataset to: {FSOCO_RAW}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "✗ Working dataset directory not found: /ml_data/perceptions/fsoco_mod. Run the preparation step and ensure the directory exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Check structure\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m FSOCO_MOD.exists():\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m      4\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✗ Working dataset directory not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFSOCO_MOD\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Run the preparation step and ensure the directory exists.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m     )\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(FSOCO_MOD.iterdir()):\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m      9\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✗ Working dataset directory is empty: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFSOCO_MOD\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Run the preparation step to copy data before proceeding.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     10\u001b[39m     )\n",
      "\u001b[31mFileNotFoundError\u001b[39m: ✗ Working dataset directory not found: /ml_data/perceptions/fsoco_mod. Run the preparation step and ensure the directory exists."
     ]
    }
   ],
   "source": [
    "# Check structure\n",
    "if not FSOCO_MOD.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"✗ Working dataset directory not found: {FSOCO_MOD}. Run the preparation step and ensure the directory exists.\"\n",
    "    )\n",
    "\n",
    "if not any(FSOCO_MOD.iterdir()):\n",
    "    raise RuntimeError(\n",
    "        f\"✗ Working dataset directory is empty: {FSOCO_MOD}. Run the preparation step to copy data before proceeding.\"\n",
    "    )\n",
    "\n",
    "working_dataset = FSOCO_MOD\n",
    "print(f\"Inspecting dataset at: {working_dataset}\")\n",
    "\n",
    "bb_path = working_dataset / 'bounding_boxes'\n",
    "if not bb_path.exists():\n",
    "    bb_path = working_dataset / 'ann'\n",
    "\n",
    "print(\"bounding_boxes/ann contents:\")\n",
    "if bb_path.exists():\n",
    "    for item in list(bb_path.iterdir())[:5]:\n",
    "        print(f\"  {item.name}\")\n",
    "else:\n",
    "    print(\"  ✗ No bounding box directory found\")\n",
    "\n",
    "img_path = working_dataset / 'images'\n",
    "if not img_path.exists():\n",
    "    img_path = working_dataset / 'img'\n",
    "\n",
    "print(\"\\nimages/img contents:\")\n",
    "if img_path.exists():\n",
    "    for item in list(img_path.iterdir())[:5]:\n",
    "        print(f\"  {item.name}\")\n",
    "else:\n",
    "    print(\"  ✗ No image directory found\")\n",
    "\n",
    "print(\"\\nmeta.json:\")\n",
    "meta_path = working_dataset / 'meta.json'\n",
    "if meta_path.exists():\n",
    "    import json\n",
    "    meta = json.load(open(meta_path))\n",
    "    print(json.dumps(meta, indent=2)[:500])\n",
    "else:\n",
    "    print(\"  ✗ meta.json not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Filter Images by Aspect Ratio\n",
    "\n",
    "Remove images that don't meet minimum aspect ratio requirements (width/height)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "✗ Working dataset directory not found: /ml_data/perceptions/fsoco_mod. Create it and rerun the preparation step before filtering.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 125\u001b[39m\n\u001b[32m    122\u001b[39m MAX_ASPECT_RATIO = \u001b[32m3.5\u001b[39m  \u001b[38;5;66;03m# Maximum width/height ratio\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m FSOCO_MOD.exists():\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m    126\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✗ Working dataset directory not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFSOCO_MOD\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Create it and rerun the preparation step before filtering.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    127\u001b[39m     )\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(FSOCO_MOD.iterdir()):\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    131\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✗ Working dataset directory is empty: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFSOCO_MOD\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Run the preparation step to copy data before filtering.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    132\u001b[39m     )\n",
      "\u001b[31mFileNotFoundError\u001b[39m: ✗ Working dataset directory not found: /ml_data/perceptions/fsoco_mod. Create it and rerun the preparation step before filtering."
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "def filter_images_by_aspect_ratio(fsoco_raw, min_ratio=1.0, max_ratio=3.0):\n",
    "    \"\"\"\n",
    "    Filter out images that don't meet aspect ratio requirements.\n",
    "    \n",
    "    Args:\n",
    "        fsoco_raw: Path to FSOCO dataset\n",
    "        min_ratio: Minimum aspect ratio (width/height)\n",
    "        max_ratio: Maximum aspect ratio (width/height)\n",
    "    \n",
    "    Returns:\n",
    "        List of filtered image paths and count of removed images\n",
    "    \"\"\"\n",
    "    img_dir = fsoco_raw / 'images'\n",
    "    if not img_dir.exists():\n",
    "        img_dir = fsoco_raw / 'img'\n",
    "    ann_dir = fsoco_raw / 'bounding_boxes'\n",
    "    if not ann_dir.exists():\n",
    "        ann_dir = fsoco_raw / 'ann'\n",
    "    \n",
    "    if not img_dir.exists():\n",
    "        print(f\"✗ Image directory not found: {img_dir}\")\n",
    "        return [], 0\n",
    "    \n",
    "    images = list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png'))\n",
    "    print(f\"Total images found: {len(images)}\")\n",
    "    \n",
    "    filtered_images = []\n",
    "    removed_images = []\n",
    "    kept_dimensions = []\n",
    "    removed_dimensions = []\n",
    "    kept_aspect_ratios = []\n",
    "    removed_aspect_ratios = []\n",
    "    \n",
    "    for img_path in tqdm(images, desc=\"Filtering images by aspect ratio\"):\n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            w, h = img.size\n",
    "            aspect_ratio = w / h\n",
    "            \n",
    "            if min_ratio <= aspect_ratio <= max_ratio:\n",
    "                filtered_images.append(img_path)\n",
    "                kept_dimensions.append((w, h))\n",
    "                kept_aspect_ratios.append(aspect_ratio)\n",
    "            else:\n",
    "                # Remove image and corresponding annotation\n",
    "                removed_images.append((img_path, aspect_ratio, w, h))\n",
    "                removed_dimensions.append((w, h))\n",
    "                removed_aspect_ratios.append(aspect_ratio)\n",
    "                \n",
    "                img_path.unlink()  # Delete image\n",
    "                \n",
    "                # Delete corresponding annotation if exists\n",
    "                ann_path = ann_dir / f\"{img_path.name}.json\"\n",
    "                if not ann_path.exists():\n",
    "                    ann_path = ann_dir / f\"{img_path.stem}.json\"\n",
    "                if ann_path.exists():\n",
    "                    ann_path.unlink()\n",
    "                \n",
    "                print(f\"Removed: {img_path.name} (aspect ratio: {aspect_ratio:.2f}, dims: {w}x{h})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_path.name}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Print detailed statistics\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"FILTERING STATISTICS\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    print(f\"\\n📊 OVERALL SUMMARY:\")\n",
    "    print(f\"  Total images processed:  {len(images)}\")\n",
    "    if images:\n",
    "        print(f\"  ✓ Images kept:           {len(filtered_images)} ({100*len(filtered_images)/len(images):.1f}%)\")\n",
    "        print(f\"  ✗ Images filtered out:   {len(removed_images)} ({100*len(removed_images)/len(images):.1f}%)\")\n",
    "    else:\n",
    "        print(\"  ✓ Images kept:           0\")\n",
    "        print(\"  ✗ Images filtered out:   0\")\n",
    "    print(f\"  Aspect ratio range:      {min_ratio} - {max_ratio}\")\n",
    "    \n",
    "    if kept_aspect_ratios:\n",
    "        print(f\"\\n📐 KEPT IMAGES - ASPECT RATIO STATS:\")\n",
    "        print(f\"  Min aspect ratio:  {min(kept_aspect_ratios):.2f}\")\n",
    "        print(f\"  Max aspect ratio:  {max(kept_aspect_ratios):.2f}\")\n",
    "        print(f\"  Avg aspect ratio:  {sum(kept_aspect_ratios)/len(kept_aspect_ratios):.2f}\")\n",
    "    \n",
    "    if kept_dimensions:\n",
    "        print(f\"\\n📏 KEPT IMAGES - DIMENSION STATS:\")\n",
    "        dim_counter = Counter(kept_dimensions)\n",
    "        top_dims = dim_counter.most_common(10)\n",
    "        print(f\"  Unique dimensions: {len(dim_counter)}\")\n",
    "        print(f\"  Top 10 dimensions:\")\n",
    "        for (w, h), count in top_dims:\n",
    "            ratio = w/h\n",
    "            print(f\"    {w}x{h} (ratio {ratio:.2f}): {count} images\")\n",
    "    \n",
    "    if removed_aspect_ratios:\n",
    "        print(f\"\\n🚫 REMOVED IMAGES - ASPECT RATIO STATS:\")\n",
    "        print(f\"  Min aspect ratio:  {min(removed_aspect_ratios):.2f}\")\n",
    "        print(f\"  Max aspect ratio:  {max(removed_aspect_ratios):.2f}\")\n",
    "        print(f\"  Avg aspect ratio:  {sum(removed_aspect_ratios)/len(removed_aspect_ratios):.2f}\")\n",
    "    \n",
    "    if removed_dimensions:\n",
    "        print(f\"\\n🚫 REMOVED IMAGES - DIMENSION STATS:\")\n",
    "        dim_counter = Counter(removed_dimensions)\n",
    "        print(f\"  Unique dimensions: {len(dim_counter)}\")\n",
    "        print(f\"  All removed dimensions:\")\n",
    "        for (w, h), count in sorted(dim_counter.items(), key=lambda x: x[1], reverse=True):\n",
    "            ratio = w/h\n",
    "            print(f\"    {w}x{h} (ratio {ratio:.2f}): {count} images\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\\n\")\n",
    "    \n",
    "    return filtered_images, len(removed_images)\n",
    "\n",
    "# Run filtering\n",
    "MIN_ASPECT_RATIO = 1.0  # Minimum width/height ratio\n",
    "MAX_ASPECT_RATIO = 3.5  # Maximum width/height ratio\n",
    "\n",
    "if not FSOCO_MOD.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"✗ Working dataset directory not found: {FSOCO_MOD}. Create it and rerun the preparation step before filtering.\"\n",
    "    )\n",
    "\n",
    "if not any(FSOCO_MOD.iterdir()):\n",
    "    raise RuntimeError(\n",
    "        f\"✗ Working dataset directory is empty: {FSOCO_MOD}. Run the preparation step to copy data before filtering.\"\n",
    "    )\n",
    "\n",
    "print(f\"Using working dataset: {FSOCO_MOD}\")\n",
    "filtered_imgs, removed = filter_images_by_aspect_ratio(\n",
    "    FSOCO_MOD,\n",
    "    min_ratio=MIN_ASPECT_RATIO,\n",
    "    max_ratio=MAX_ASPECT_RATIO\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Convert Labels: Supervisely → YOLO\n",
    "\n",
    "YOLO format: `<class_id> <x_center> <y_center> <width> <height>` (all normalized 0-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Add code for conversion\n",
    "\n",
    "def convert_supervisely_to_yolo(fsoco_raw, fsoco_yolo, split=(0.8, 0.1, 0.1)):\n",
    "    \"\"\"Convert FSOCO Supervisely format to YOLO format.\"\"\"\n",
    "    \n",
    "    # Create directories\n",
    "    for s in ['train', 'val', 'test']:\n",
    "        (fsoco_yolo / 'images' / s).mkdir(parents=True, exist_ok=True)\n",
    "        (fsoco_yolo / 'labels' / s).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Load class names from meta.json (only rectangle classes for bounding boxes)\n",
    "    meta = json.load(open(fsoco_raw / 'meta.json'))\n",
    "    classes = [c['title'] for c in meta['classes'] if c['shape'] == 'rectangle']\n",
    "    class_map = {name: i for i, name in enumerate(classes)}\n",
    "    print(f\"Classes (bounding boxes only): {classes}\\n\")\n",
    "    \n",
    "    # Get all annotation files\n",
    "    ann_dir = fsoco_raw / 'bounding_boxes'\n",
    "    img_dir = fsoco_raw / 'images'\n",
    "    \n",
    "    ann_files = list(ann_dir.glob('*.json'))\n",
    "    print(f\"Total annotations: {len(ann_files)}\")\n",
    "    \n",
    "    # Split dataset\n",
    "    random.shuffle(ann_files)\n",
    "    n1 = int(len(ann_files) * split[0])\n",
    "    n2 = int(len(ann_files) * (split[0] + split[1]))\n",
    "    splits = {\n",
    "        'train': ann_files[:n1],\n",
    "        'val': ann_files[n1:n2],\n",
    "        'test': ann_files[n2:]\n",
    "    }\n",
    "    \n",
    "    # Convert each split\n",
    "    for split_name, anns in splits.items():\n",
    "        print(f\"\\nConverting {split_name}: {len(anns)} images\")\n",
    "        \n",
    "        for ann_path in tqdm(anns):\n",
    "            # Find corresponding image (handle both .jpg and .png)\n",
    "            img_name = ann_path.stem  # e.g., \"amz_00588.jpg\" from \"amz_00588.jpg.json\"\n",
    "            img_path = img_dir / img_name\n",
    "            \n",
    "            if not img_path.exists():\n",
    "                # Try alternate extension\n",
    "                alt_name = img_name.replace('.jpg', '.png') if '.jpg' in img_name else img_name.replace('.png', '.jpg')\n",
    "                img_path = img_dir / alt_name\n",
    "                if not img_path.exists():\n",
    "                    continue\n",
    "            \n",
    "            # Read image dimensions\n",
    "            img = Image.open(img_path)\n",
    "            w, h = img.size\n",
    "            \n",
    "            # Read annotations\n",
    "            ann = json.load(open(ann_path))\n",
    "            \n",
    "            # Convert to YOLO format\n",
    "            yolo_labels = []\n",
    "            for obj in ann.get('objects', []):\n",
    "                cls = obj['classTitle']\n",
    "                if cls not in class_map or obj['geometryType'] != 'rectangle':\n",
    "                    continue\n",
    "                \n",
    "                points = obj['points']['exterior']\n",
    "                x1, y1 = points[0]\n",
    "                x2, y2 = points[1]\n",
    "                \n",
    "                # Convert to YOLO format (center_x, center_y, width, height) normalized\n",
    "                x_center = ((x1 + x2) / 2) / w\n",
    "                y_center = ((y1 + y2) / 2) / h\n",
    "                width = abs(x2 - x1) / w\n",
    "                height = abs(y2 - y1) / h\n",
    "                \n",
    "                # Clamp to [0, 1]\n",
    "                x_center = max(0, min(1, x_center))\n",
    "                y_center = max(0, min(1, y_center))\n",
    "                width = max(0, min(1, width))\n",
    "                height = max(0, min(1, height))\n",
    "                \n",
    "                yolo_labels.append(f\"{class_map[cls]} {x_center:.6f} {y_center:.6f} {width:.6f} {height:.6f}\")\n",
    "            \n",
    "            # Save image and label\n",
    "            if yolo_labels:  # Only save if there are labels\n",
    "                shutil.copy(img_path, fsoco_yolo / 'images' / split_name / img_path.name)\n",
    "                with open(fsoco_yolo / 'labels' / split_name / f\"{img_path.stem}.txt\", 'w') as f:\n",
    "                    f.write('\\n'.join(yolo_labels))\n",
    "    \n",
    "    print(\"\\n✓ Conversion complete\")\n",
    "    print(f\"\\nDataset splits:\")\n",
    "    for split_name in ['train', 'val', 'test']:\n",
    "        n_imgs = len(list((fsoco_yolo / 'images' / split_name).glob('*.[jp][pn]g')))\n",
    "        print(f\"  {split_name}: {n_imgs} images\")\n",
    "    \n",
    "    return classes\n",
    "\n",
    "# Run conversion\n",
    "if FSOCO_RAW.exists() and any(FSOCO_RAW.iterdir()):\n",
    "    classes = convert_supervisely_to_yolo(FSOCO_RAW, FSOCO_YOLO)\n",
    "else:\n",
    "    print(\"Download dataset first!\")\n",
    "    classes = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Dataset Config (YAML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "if classes:\n",
    "    config = {\n",
    "        'path': str(FSOCO_YOLO.absolute()),\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/val',\n",
    "        'test': 'images/test',\n",
    "        'nc': len(classes),\n",
    "        'names': classes\n",
    "    }\n",
    "    \n",
    "    config_path = FSOCO_YOLO / 'fsoco.yaml'\n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "    \n",
    "    print(\"✓ Config saved:\", config_path)\n",
    "    print(yaml.dump(config, default_flow_style=False))\n",
    "else:\n",
    "    print(\"Convert dataset first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train YOLOv5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "\n",
    "import torch\n",
    "MODEL = 'yolov5n'  # Options: yolov5n, yolov5s, yolov5m, yolov5l, yolov5x\n",
    "EPOCHS = 10\n",
    "BATCH = 2\n",
    "IMG_SIZE = 640\n",
    "DEVICE = \"0\" if torch.cuda.is_available() else \"cpu\"\n",
    "rect = True \n",
    "print(f\"Model: {MODEL}, Epochs: {EPOCHS}, Batch: {BATCH}, Image size: {IMG_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "\n",
    "%cd $YOLO_DIR\n",
    "!python train.py \\\n",
    "    --img {IMG_SIZE} \\\n",
    "    --batch {BATCH} \\\n",
    "    --epochs {EPOCHS} \\\n",
    "    --data {config_path} \\\n",
    "    --weights {MODEL}.pt \\\n",
    "    --name fsoco_{MODEL} \\\n",
    "    --rect {rect} \\\n",
    "    --cache \\\n",
    "    --device {DEVICE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IPImage\n",
    "\n",
    "# Find latest run\n",
    "runs = sorted((YOLO_DIR / 'runs' / 'train').glob('fsoco_*'), key=lambda x: x.stat().st_mtime)\n",
    "if runs:\n",
    "    latest = runs[-1]\n",
    "    print(f\"Training run: {latest.name}\\n\")\n",
    "    \n",
    "    # Show results\n",
    "    results_img = latest / 'results.png'\n",
    "    if results_img.exists():\n",
    "        display(IPImage(filename=str(results_img)))\n",
    "    \n",
    "    print(f\"\\nWeights: {latest / 'weights' / 'best.pt'}\")\n",
    "else:\n",
    "    print(\"No training runs found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get trained weights\n",
    "if runs:\n",
    "    weights = runs[-1] / 'weights' / 'best.pt'\n",
    "    test_imgs = FSOCO_YOLO / 'images' / 'test'\n",
    "    \n",
    "    if weights.exists() and test_imgs.exists():\n",
    "        print(\"Running inference...\\n\")\n",
    "        \n",
    "        !python {YOLO_DIR}/detect.py \\\n",
    "            --weights {weights} \\\n",
    "            --source {test_imgs} \\\n",
    "            --img {IMG_SIZE} \\\n",
    "            --conf 0.25 \\\n",
    "            --name fsoco_inference \\\n",
    "            --max-det 100\n",
    "        \n",
    "        # Show results\n",
    "        detect_runs = sorted((YOLO_DIR / 'runs' / 'detect').glob('fsoco_inference*'), \n",
    "                           key=lambda x: x.stat().st_mtime)\n",
    "        if detect_runs:\n",
    "            print(f\"\\nResults saved to: {detect_runs[-1]}\")\n",
    "            \n",
    "            # Display first result\n",
    "            results = list(detect_runs[-1].glob('*.jpg'))[:1]\n",
    "            for r in results:\n",
    "                display(IPImage(filename=str(r), width=800))\n",
    "    else:\n",
    "        print(\"Missing weights or test images\")\n",
    "else:\n",
    "    print(\"Train model first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done!\n",
    "\n",
    "**Your model is trained and ready to use.**\n",
    "\n",
    "**Quick commands:**\n",
    "```bash\n",
    "# Inference on images\n",
    "python yolov5/detect.py --weights runs/train/fsoco_yolov5s/weights/best.pt --source /path/to/images\n",
    "\n",
    "# Inference on video\n",
    "python yolov5/detect.py --weights runs/train/fsoco_yolov5s/weights/best.pt --source video.mp4\n",
    "\n",
    "# Inference on webcam\n",
    "python yolov5/detect.py --weights runs/train/fsoco_yolov5s/weights/best.pt --source 0\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
